<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-leetcode-69开方" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/07/leetcode-69%E5%BC%80%E6%96%B9/" class="article-date">
  <time datetime="2019-01-07T07:30:18.000Z" itemprop="datePublished">2019-01-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/07/leetcode-69%E5%BC%80%E6%96%B9/">leetcode_69</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>点评：这道评分为easy的题就是你的盲区。。。</p>
<ol>
<li><p>题目描述<br><img src="1.png" alt="1"></p>
</li>
<li><p>分析：考点在于int的溢出！<br>所以变量用long long </p>
</li>
<li><p>正确解法：<br>1）没有思考的暴力法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    int mySqrt(int x) &#123;</span><br><span class="line">        if (x&#x3D;&#x3D;0)  return 0;</span><br><span class="line">        if(x&#x3D;&#x3D;1)  return 1;</span><br><span class="line">        for(int i&#x3D;0;i&lt;x;i++)&#123;</span><br><span class="line">            if((i+1)*(i+1)&gt;x||i*i&#x3D;&#x3D;x||(i+1)*(i+1)&lt;i*i)</span><br><span class="line">                return i;</span><br><span class="line">        &#125;</span><br><span class="line">        return 1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>2）有思考的暴力法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">  int mySqrt(int x) &#123;</span><br><span class="line">    if (x &lt;&#x3D; 1) return x;</span><br><span class="line">    for (long long s &#x3D; 1; s &lt;&#x3D; x; ++s)</span><br><span class="line">      if (s * s &gt; x) return s - 1;</span><br><span class="line">    return -1;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>


<ol start="3">
<li>有思考的二分法<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    int mySqrt(int x) &#123;</span><br><span class="line">        if(x&#x3D;&#x3D;0) return 0;</span><br><span class="line">        if(x&#x3D;&#x3D;1)  return 1;</span><br><span class="line">        long l&#x3D;1;</span><br><span class="line">        long r&#x3D;x;</span><br><span class="line">        while(l&lt;r)&#123;</span><br><span class="line">            long m&#x3D;(l+r)&#x2F;2;</span><br><span class="line">            if(m*m&gt;x)&#123;</span><br><span class="line">                r&#x3D;m;</span><br><span class="line">            &#125;</span><br><span class="line">            if(m*m&lt;x)&#123;</span><br><span class="line">                l&#x3D;m+1;</span><br><span class="line">            &#125;</span><br><span class="line">            if(m*m&#x3D;&#x3D;x||((m+1)*(m+1)&gt;x&amp;&amp;m*m&lt;x))&#123;</span><br><span class="line">                return m;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/01/07/leetcode-69%E5%BC%80%E6%96%B9/" data-id="ck7t8trem000ck6s6hapi2bzn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%88%B7%E9%A2%98/" rel="tag">刷题</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-decode-ways" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/07/decode-ways/" class="article-date">
  <time datetime="2019-01-07T04:49:50.000Z" itemprop="datePublished">2019-01-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/07/decode-ways/">decode_ways</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li><p>题目描述：<br><img src="1.png" alt="1"><br><img src="2.png" alt="2"></p>
</li>
<li><p>初探印象：<br>wrong code:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    int numDecodings(string str) &#123;</span><br><span class="line">        int c&#x3D;1;</span><br><span class="line">        int k&#x3D;1;</span><br><span class="line">        int len&#x3D;str.size();</span><br><span class="line">        &#x2F;&#x2F;bool f&#x3D;true;</span><br><span class="line">        int w;</span><br><span class="line">        for(w&#x3D;0;w&lt;len;w++)&#123;</span><br><span class="line">            if(str[w]&#x3D;&#x3D;&#39;0&#39;)&#123;</span><br><span class="line">                return 0;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#x2F;&#x2F;  str&#x3D;str.substr(w);</span><br><span class="line">        len&#x3D;str.size();</span><br><span class="line">        if(len&#x3D;&#x3D;0)    return 0;</span><br><span class="line">        if(len&#x3D;&#x3D;1)   return c;</span><br><span class="line">        for(int i&#x3D;0,j&#x3D;i+1;j&lt;len;i++,j&#x3D;i+1)&#123;</span><br><span class="line">            if(len%2&#x3D;&#x3D;0&amp;&amp;((str[i]-&#39;0&#39;)*10+(str[j]-&#39;0&#39;))&lt;&#x3D;26&amp;&amp;(str[j]&gt;&#39;0&#39;))&#123;</span><br><span class="line">                c&#x3D;c*2;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            if(len%2&#x3D;&#x3D;1&amp;&amp;((str[i]-&#39;0&#39;)*10+(str[j]-&#39;0&#39;))&lt;&#x3D;26)&#123;</span><br><span class="line">                c&#x3D;c*2;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        if(len%2&#x3D;&#x3D;0)  return c;</span><br><span class="line">        else  return c+1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>分析：自己没能找到一个合理的逻辑，一直根据wrong error的错误的测试用例来缝缝补补，乱花渐欲迷人眼，耗尽了耐心，不可取。还是要找到一个正确的思路要紧。</p>
<ol start="3">
<li>正确的方法：<br> 用dynamic programming<br> 类比斐波那契数列<br> 解码是有规律的，所以我们可以尝试动态规划。假设数组dp[i]表示从头到字符串的第i位，一共有多少种解码方法<br> dp[i]对应的字符为s[i-1].</li>
</ol>
<p>分析：<br>精准的找到三个出口：<br>0. if(s[i-1]==’0’&amp;&amp;s[i-2]==’0’)  return 0;   //连着出现两个0的情况</p>
<ol>
<li>if((s[i-2]-‘0’)*10+(s[i-1]-‘0’)&lt;=26&amp;&amp;(s[i-1]!=’0’&amp;&amp;s[i-2]!=’0’)){<pre><code>dp[i]= dp[i-1]+dp[i-2];        //符合斐波那契数列的情况</code></pre></li>
<li>else if((s[i-2]-‘0’)*10+(s[i-1]-‘0’)&gt;26&amp;&amp;s[i-1]==’0’)  return 0;    //数列中出现为0的情况</li>
<li>else dp[i]=s[i-1]==’0’?dp[i-2]:dp[i-1];    //其他不构成斐波那契数列的绝大多数情况。</li>
</ol>
<p>correct code:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    int numDecodings(string s) &#123;</span><br><span class="line">        int len&#x3D;s.size();</span><br><span class="line">        if(len&#x3D;&#x3D;0) return 0;</span><br><span class="line">        vector&lt;int&gt; dp(len+1,0);</span><br><span class="line">        dp[0]&#x3D;1;</span><br><span class="line">        if(s[0]&#x3D;&#x3D;&#39;0&#39;)  return 0;</span><br><span class="line">        dp[1]&#x3D;1;</span><br><span class="line">        for(int i&#x3D;2;i&lt;&#x3D;len;i++)&#123;</span><br><span class="line">            if(s[i-1]&#x3D;&#x3D;&#39;0&#39;&amp;&amp;s[i-2]&#x3D;&#x3D;&#39;0&#39;)  return 0;</span><br><span class="line">            if((s[i-2]-&#39;0&#39;)*10+(s[i-1]-&#39;0&#39;)&lt;&#x3D;26&amp;&amp;(s[i-1]!&#x3D;&#39;0&#39;&amp;&amp;s[i-2]!&#x3D;&#39;0&#39;))&#123;</span><br><span class="line">                dp[i]&#x3D; dp[i-1]+dp[i-2];</span><br><span class="line">            &#125;</span><br><span class="line">            else if((s[i-2]-&#39;0&#39;)*10+(s[i-1]-&#39;0&#39;)&gt;26&amp;&amp;s[i-1]&#x3D;&#x3D;&#39;0&#39;)</span><br><span class="line">                return 0;</span><br><span class="line">            else </span><br><span class="line">                dp[i]&#x3D;s[i-1]&#x3D;&#x3D;&#39;0&#39;?dp[i-2]:dp[i-1];</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">        return dp[len];</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/01/07/decode-ways/" data-id="ck7t8trei0004k6s60faw2bch" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%88%B7%E9%A2%98/" rel="tag">刷题</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-两个排序数组的中位数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/04/%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/" class="article-date">
  <time datetime="2019-01-04T07:55:42.000Z" itemprop="datePublished">2019-01-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/04/%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/">两个排序数组的中位数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>没做一题，都会被打击好多次，但是算法是一定要学习的！</p>
<hr>
<p>！<a href="1.png">题目描述</a></p>
<p>代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    double findMedianSortedArrays(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123;</span><br><span class="line">        int m&#x3D;nums1.size();</span><br><span class="line">        int n&#x3D;nums2.size();</span><br><span class="line">        if((m+n)%2&#x3D;&#x3D;1)&#123;</span><br><span class="line">            return Help(nums1,0,nums2,0,(m+n)&#x2F;2+1);</span><br><span class="line">        &#125;</span><br><span class="line">        else&#123;</span><br><span class="line">            return (double)((Help(nums1,0,nums2,0,(m+n)&#x2F;2)+Help(nums1,0,nums2,0,(m+n)&#x2F;2+1))&#x2F;2);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    double Help(vector&lt;int&gt; a,int offset_a,vector&lt;int&gt; b,int offset_b, int k)&#123;</span><br><span class="line">        int m&#x3D;a.size()-offset_a;</span><br><span class="line">        int n&#x3D;b.size()-offset_b;</span><br><span class="line">        if(n&lt;m)&#123;</span><br><span class="line">            return Help(b,offset_b,a,offset_a,k);</span><br><span class="line">        &#125;</span><br><span class="line">        if(m&#x3D;&#x3D;0)  return b[offset_b+k-1];</span><br><span class="line">        if(k&#x3D;&#x3D;1)   return min(a[offset_a],b[offset_b]);</span><br><span class="line">        int pa&#x3D;min(k&#x2F;2,m);</span><br><span class="line">        int pb&#x3D;k-pa;</span><br><span class="line">        if(a[offset_a+pa-1]&lt;b[offset_b+pb-1])   return Help(a,offset_a+pa,b,offset_b,k-pa);</span><br><span class="line">        else    return Help(a,offset_a,b,offset_b+pb,k-pb);&#x2F;&#x2F;一定要细心哦，偏移量处处都要有！</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>参考文献[特别鸣谢]：<br><a href="https://zhuanlan.zhihu.com/p/27104702" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27104702</a><br><a href="https://www.cnblogs.com/lichen782/p/leetcode_Median_of_Two_Sorted_Arrays.html" target="_blank" rel="noopener">https://www.cnblogs.com/lichen782/p/leetcode_Median_of_Two_Sorted_Arrays.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/01/04/%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/" data-id="ck7t8trew0011k6s67vcufthg" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%88%B7%E9%A2%98/" rel="tag">刷题</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-最大回文子串" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/04/%E6%9C%80%E5%A4%A7%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/" class="article-date">
  <time datetime="2019-01-04T03:03:10.000Z" itemprop="datePublished">2019-01-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/01/04/%E6%9C%80%E5%A4%A7%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/">最大回文子串</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>！<a href="1.png">最大回文子串题目描述</a></p>
<p>解决方案：<br>1.用动态规划：时间复杂度O(N^2),空间复杂度O(N^2)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    string longestPalindrome(string s) &#123;</span><br><span class="line">        string res;</span><br><span class="line">        if(s.empty())  return &quot;&quot;;</span><br><span class="line">        if(s.size()&#x3D;&#x3D;1)  return s;</span><br><span class="line">        int maxlen&#x3D;1;</span><br><span class="line">        int start&#x3D;0;</span><br><span class="line">        bool dp[1000][1000]&#x3D;&#123;false&#125;;</span><br><span class="line">        for(int len&#x3D;0;len&lt;s.size();len++)&#123;&#x2F;&#x2F;len表示回文首末间的距离</span><br><span class="line">            for(int i&#x3D;0;i&lt;s.size()-len;i++)&#123;&#x2F;&#x2F;i表示回文首位</span><br><span class="line">                if(len&#x3D;&#x3D;0)</span><br><span class="line">                    dp[i][len]&#x3D;true;</span><br><span class="line">                else if(s[i]&#x3D;&#x3D;s[i+len])&#123;</span><br><span class="line">                    if(len&#x3D;&#x3D;1)  dp[i][len]&#x3D;true;</span><br><span class="line">                    else&#123;</span><br><span class="line">                        dp[i][len]&#x3D;dp[i+1][len-2];</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;  </span><br><span class="line">                </span><br><span class="line">                else</span><br><span class="line">                    dp[i][len]&#x3D;false;</span><br><span class="line">                if(dp[i][len]&amp;&amp;len+1&gt;maxlen)&#123;</span><br><span class="line">                    maxlen&#x3D;len+1;</span><br><span class="line">                    start&#x3D;i;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return s.substr(start,maxlen);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>


<ol start="2">
<li>用中心扩散法——使得时间复杂度为O(N^2),空间复杂度为O(1)</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">public:</span><br><span class="line">    string longestPalindrome(string s) &#123;</span><br><span class="line">        if(s.empty())  return &quot;&quot;;</span><br><span class="line">        if(s.size()&#x3D;&#x3D;1)  return s;</span><br><span class="line">        for(int i&#x3D;0;i&lt;s.size();i++)&#123;</span><br><span class="line">            Help(s,i,0);  &#x2F;&#x2F;eg: abcbd</span><br><span class="line">            Help(s,i,1);  &#x2F;&#x2F;eg: abccbd</span><br><span class="line">        &#125;</span><br><span class="line">        return longer;</span><br><span class="line">    &#125;</span><br><span class="line">    void Help(string s, int cen, int offset)&#123;</span><br><span class="line">        int left&#x3D;cen;</span><br><span class="line">        int right&#x3D;cen+offset;</span><br><span class="line">        while(left&gt;&#x3D;0&amp;&amp;right&lt;s.size()&amp;&amp;s[left]&#x3D;&#x3D;s[right])&#123;</span><br><span class="line">            left--;</span><br><span class="line">            right++;</span><br><span class="line">            &#125;</span><br><span class="line">        left++;</span><br><span class="line">        right--;        </span><br><span class="line">        string tmp&#x3D;s.substr(left,right-left+1);</span><br><span class="line">        if(right-left+1&gt;longer.size())&#123;</span><br><span class="line">            longer&#x3D;tmp;</span><br><span class="line">        &#125;      </span><br><span class="line">    &#125;</span><br><span class="line">private:</span><br><span class="line">    string longer;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>马拉车算法，时间复杂度为O(N)<br>——————日后再看。</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/01/04/%E6%9C%80%E5%A4%A7%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2/" data-id="ck7t8trf0001ck6s6hcuqe2gq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%88%B7%E9%A2%98/" rel="tag">刷题</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="coursera联系——optimization-GD" class="article article-type-coursera联系——optimization" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/17/GD/" class="article-date">
  <time datetime="2018-12-17T08:13:42.000Z" itemprop="datePublished">2018-12-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/17/GD/">GD</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Optimization-Methods"><a href="#Optimization-Methods" class="headerlink" title="Optimization Methods"></a>Optimization Methods</h1><p>Until now, you’ve always used Gradient Descent to update the parameters and minimize the cost. In this notebook, you will learn more advanced optimization methods that can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result. </p>
<p>Gradient descent goes “downhill” on a cost function $J$. Think of it as trying to do this:<br><img src="1.png" alt=""><br> <strong>Figure 1</strong> </u>: <strong>Minimizing the cost is like finding the lowest point in a hilly landscape</strong><br> At each step of the training, you update your parameters following a certain direction to try to get to the lowest possible point. </center></caption></p>
<p><strong>Notations</strong>: As usual, $\frac{\partial J}{\partial a } = $ <code>da</code> for any variable <code>a</code>.</p>
<p>To get started, run the following code to import the libraries you will need.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation</span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure>


<h2 id="1-Gradient-Descent"><a href="#1-Gradient-Descent" class="headerlink" title="1 - Gradient Descent"></a>1 - Gradient Descent</h2><p>A simple optimization method in machine learning is gradient descent (GD). When you take gradient steps with respect to all $m$ examples on each step, it is also called Batch Gradient Descent. </p>
<p><strong>Warm-up exercise</strong>: Implement the gradient descent update rule. The  gradient descent rule is, for $l = 1, …, L$:<br>$$ W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \tag{1}$$<br>$$ b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} \tag{2}$$</p>
<p>where L is the number of layers and $\alpha$ is the learning rate. All parameters should be stored in the <code>parameters</code> dictionary. Note that the iterator <code>l</code> starts at 0 in the <code>for</code> loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift <code>l</code> to <code>l+1</code> when coding.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters_with_gd</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_gd</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using one step of gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters to be updated:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients to update each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] =  parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)]- learning_rate * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] =  parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)]- learning_rate * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads, learning_rate = update_parameters_with_gd_test_case()</span><br><span class="line"></span><br><span class="line">parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[ 1.63535156 -0.62320365 -0.53718766]
 [-1.07799357  0.85639907 -2.29470142]]
b1 = [[ 1.74604067]
 [-0.75184921]]
W2 = [[ 0.32171798 -0.25467393  1.46902454]
 [-2.05617317 -0.31554548 -0.3756023 ]
 [ 1.1404819  -1.09976462 -0.1612551 ]]
b2 = [[-0.88020257]
 [ 0.02561572]
 [ 0.57539477]]</code></pre><p><strong>Expected Output</strong>:</p>
<table> 
    <tr>
    <td > **W1** </td> 
           <td > [[ 1.63535156 -0.62320365 -0.53718766]
 [-1.07799357  0.85639907 -2.29470142]] </td> 
    </tr> 

<pre><code>&lt;tr&gt;
&lt;td &gt; **b1** &lt;/td&gt; 
       &lt;td &gt; [[ 1.74604067]</code></pre><p> [-0.75184921]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **W2** &lt;/td&gt; 
       &lt;td &gt; [[ 0.32171798 -0.25467393  1.46902454]</code></pre><p> [-2.05617317 -0.31554548 -0.3756023 ]<br> [ 1.1404819  -1.09976462 -0.1612551 ]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **b2** &lt;/td&gt; 
       &lt;td &gt; [[-0.88020257]</code></pre><p> [ 0.02561572]<br> [ 0.57539477]] </td><br>    </tr> </p>
</table>


<p>A variant of this is Stochastic Gradient Descent (SGD), which is equivalent to mini-batch gradient descent where each mini-batch has just 1 example. The update rule that you have just implemented does not change. What changes is that you would be computing gradients on just one training example at a time, rather than on the whole training set. The code examples below illustrate the difference between stochastic gradient descent and (batch) gradient descent. </p>
<ul>
<li><strong>(Batch) Gradient Descent</strong>:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="comment"># Forward propagation</span></span><br><span class="line">    a, caches = forward_propagation(X, parameters)</span><br><span class="line">    <span class="comment"># Compute cost.</span></span><br><span class="line">    cost = compute_cost(a, Y)</span><br><span class="line">    <span class="comment"># Backward propagation.</span></span><br><span class="line">    grads = backward_propagation(a, caches, parameters)</span><br><span class="line">    <span class="comment"># Update parameters.</span></span><br><span class="line">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Stochastic Gradient Descent</strong>:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, m):</span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(a, Y[:,j])</span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = backward_propagation(a, caches, parameters)</span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>


<p>In Stochastic Gradient Descent, you use only 1 training example before updating the gradients. When the training set is large, SGD can be faster. But the parameters will “oscillate” toward the minimum rather than converge smoothly. Here is an illustration of this:<br><img src="2.png" alt=""><br><strong>Figure 1</strong> </u><font color='purple'>  : <strong>SGD vs GD</strong><br> “+” denotes a minimum of the cost. SGD leads to many oscillations to reach convergence. But each step is a lot faster to compute for SGD than for GD, as it uses only one training example (vs. the whole batch for GD). </center></caption></p>
<p><strong>Note</strong> also that implementing SGD requires 3 for-loops in total:</p>
<ol>
<li>Over the number of iterations</li>
<li>Over the $m$ training examples</li>
<li>Over the layers (to update all parameters, from $(W^{[1]},b^{[1]})$ to $(W^{[L]},b^{[L]})$)</li>
</ol>
<p>In practice, you’ll often get faster results if you do not use neither the whole training set, nor only one training example, to perform each update. Mini-batch gradient descent uses an intermediate number of examples for each step. With mini-batch gradient descent, you loop over the mini-batches instead of looping over individual training examples.<br><img src="3.png" alt=""><br> <strong>Figure 2</strong> </u>: <font color='purple'>  <strong>SGD vs Mini-Batch GD</strong><br> “+” denotes a minimum of the cost. Using mini-batches in your optimization algorithm often leads to faster optimization. </center></caption></p>
<font color='blue'>
**What you should remember**:
- The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.
- You have to tune a learning rate hyperparameter $\alpha$.
- With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).

<h2 id="2-Mini-Batch-Gradient-descent"><a href="#2-Mini-Batch-Gradient-descent" class="headerlink" title="2 - Mini-Batch Gradient descent"></a>2 - Mini-Batch Gradient descent</h2><p>Let’s learn how to build mini-batches from the training set (X, Y).</p>
<p>There are two steps:</p>
<ul>
<li><p><strong>Shuffle</strong>: Create a shuffled version of the training set (X, Y) as shown below. Each column of X and Y represents a training example. Note that the random shuffling is done synchronously between X and Y. Such that after the shuffling the $i^{th}$ column of X is the example corresponding to the $i^{th}$ label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches.<br><img src="4.png" alt=""></p>
</li>
<li><p><strong>Partition</strong>: Partition the shuffled (X, Y) into mini-batches of size <code>mini_batch_size</code> (here 64). Note that the number of training examples is not always divisible by <code>mini_batch_size</code>. The last mini batch might be smaller, but you don’t need to worry about this. When the final mini-batch is smaller than the full <code>mini_batch_size</code>, it will look like this:<br><img src="5.png" alt=""></p>
</li>
</ul>
<p><strong>Exercise</strong>: Implement <code>random_mini_batches</code>. We coded the shuffling part for you. To help you with the partitioning step, we give you the following code that selects the indexes for the $1^{st}$ and $2^{nd}$ mini-batches:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">first_mini_batch_X = shuffled_X[:, <span class="number">0</span> : mini_batch_size]</span><br><span class="line">second_mini_batch_X = shuffled_X[:, mini_batch_size : <span class="number">2</span> * mini_batch_size]</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>Note that the last mini-batch might end up smaller than <code>mini_batch_size=64</code>. Let $\lfloor s \rfloor$ represents $s$ rounded down to the nearest integer (this is <code>math.floor(s)</code> in Python). If the total number of examples is not a multiple of <code>mini_batch_size=64</code> then there will be $\lfloor \frac{m}{mini_batch_size}\rfloor$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be ($m-mini__batch__size \times \lfloor \frac{m}{mini_batch_size}\rfloor$). </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: random_mini_batches</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_mini_batches</span><span class="params">(X, Y, mini_batch_size = <span class="number">64</span>, seed = <span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a list of random minibatches from (X, Y)    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    mini_batch_size -- size of the mini-batches, integer    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span></span><br><span class="line"><span class="string">    """</span>  </span><br><span class="line">    np.random.seed(seed)            <span class="comment"># To make your "random" minibatches the same as ours</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                  <span class="comment"># number of training examples</span></span><br><span class="line">    mini_batches = []        </span><br><span class="line">    <span class="comment"># Step 1: Shuffle (X, Y)</span></span><br><span class="line">    permutation = list(np.random.permutation(m))</span><br><span class="line">    shuffled_X = X[:, permutation]</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((<span class="number">1</span>,m))</span><br><span class="line">    <span class="comment"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span></span><br><span class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) <span class="comment"># number of mini batches of size mini_batch_size in your partitionning</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, num_complete_minibatches):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        mini_batch_X = shuffled_X[:,k*mini_batch_size:(k+<span class="number">1</span>)*mini_batch_size]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:,k*mini_batch_size:(k+<span class="number">1</span>)*mini_batch_size]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Handling the end case (last mini-batch &lt; mini_batch_size)</span></span><br><span class="line">    <span class="keyword">if</span> m % mini_batch_size != <span class="number">0</span>:</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        mini_batch_X = shuffled_X[:,num_complete_minibatches*mini_batch_size:m]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:,num_complete_minibatches*mini_batch_size:m]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mini_batches</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()</span><br><span class="line">mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 1st mini_batch_X: "</span> + str(mini_batches[<span class="number">0</span>][<span class="number">0</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 2nd mini_batch_X: "</span> + str(mini_batches[<span class="number">1</span>][<span class="number">0</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 3rd mini_batch_X: "</span> + str(mini_batches[<span class="number">2</span>][<span class="number">0</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 1st mini_batch_Y: "</span> + str(mini_batches[<span class="number">0</span>][<span class="number">1</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 2nd mini_batch_Y: "</span> + str(mini_batches[<span class="number">1</span>][<span class="number">1</span>].shape)) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 3rd mini_batch_Y: "</span> + str(mini_batches[<span class="number">2</span>][<span class="number">1</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"mini batch sanity check: "</span> + str(mini_batches[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>:<span class="number">3</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>shape of the 1st mini_batch_X: (12288, 64)
shape of the 2nd mini_batch_X: (12288, 64)
shape of the 3rd mini_batch_X: (12288, 20)
shape of the 1st mini_batch_Y: (1, 64)
shape of the 2nd mini_batch_Y: (1, 64)
shape of the 3rd mini_batch_Y: (1, 20)
mini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]</code></pre><p><strong>Expected Output</strong>:</p>
<table style="width:50%"> 
    <tr>
    <td > **shape of the 1st mini_batch_X** </td> 
           <td > (12288, 64) </td> 
    </tr> 

<pre><code>&lt;tr&gt;
&lt;td &gt; **shape of the 2nd mini_batch_X** &lt;/td&gt; 
       &lt;td &gt; (12288, 64) &lt;/td&gt; 
&lt;/tr&gt; 

&lt;tr&gt;
&lt;td &gt; **shape of the 3rd mini_batch_X** &lt;/td&gt; 
       &lt;td &gt; (12288, 20) &lt;/td&gt; 
&lt;/tr&gt;
&lt;tr&gt;
&lt;td &gt; **shape of the 1st mini_batch_Y** &lt;/td&gt; 
       &lt;td &gt; (1, 64) &lt;/td&gt; 
&lt;/tr&gt; 
&lt;tr&gt;
&lt;td &gt; **shape of the 2nd mini_batch_Y** &lt;/td&gt; 
       &lt;td &gt; (1, 64) &lt;/td&gt; 
&lt;/tr&gt; 
&lt;tr&gt;
&lt;td &gt; **shape of the 3rd mini_batch_Y** &lt;/td&gt; 
       &lt;td &gt; (1, 20) &lt;/td&gt; 
&lt;/tr&gt; 
&lt;tr&gt;
&lt;td &gt; **mini batch sanity check** &lt;/td&gt; 
       &lt;td &gt; [ 0.90085595 -0.7612069   0.2344157 ] &lt;/td&gt; 
&lt;/tr&gt;</code></pre></table>

<font color='blue'>
**What you should remember**:
- Shuffling and Partitioning are the two steps required to build mini-batches
- Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.

<h2 id="3-Momentum"><a href="#3-Momentum" class="headerlink" title="3 - Momentum"></a>3 - Momentum</h2><p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations. </p>
<p>Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.<br><img src="6.png" alt=""><br><strong>Figure 3</strong></u><font color='purple'>: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence $v$ and then take a step in the direction of $v$.<br> <font color='black'> </center></p>
<p><strong>Exercise</strong>: Initialize the velocity. The velocity, $v$, is a python dictionary that needs to be initialized with arrays of zeros. Its keys are the same as those in the <code>grads</code> dictionary, that is:<br>for $l =1,…,L$:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["W" + str(l+1)])</span></span><br><span class="line">v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["b" + str(l+1)])</span></span><br></pre></td></tr></table></figure>
<p><strong>Note</strong> that the iterator l starts at 0 in the for loop while the first parameters are v[“dW1”] and v[“db1”] (that’s a “one” on the superscript). This is why we are shifting l to l+1 in the <code>for</code> loop.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_velocity</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_velocity</span><span class="params">(parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes the velocity as a python dictionary with:</span></span><br><span class="line"><span class="string">                - keys: "dW1", "db1", ..., "dWL", "dbL" </span></span><br><span class="line"><span class="string">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity.</span></span><br><span class="line"><span class="string">                    v['dW' + str(l)] = velocity of dWl</span></span><br><span class="line"><span class="string">                    v['db' + str(l)] = velocity of dbl</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize velocity</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)].shape))</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros(( parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)].shape))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_velocity_test_case()</span><br><span class="line"></span><br><span class="line">v = initialize_velocity(parameters)</span><br><span class="line">print(<span class="string">"v[\"dW1\"] = "</span> + str(v[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db1\"] = "</span> + str(v[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW2\"] = "</span> + str(v[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db2\"] = "</span> + str(v[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>v[&quot;dW1&quot;] = [[ 0.  0.  0.]
 [ 0.  0.  0.]]
v[&quot;db1&quot;] = [[ 0.]
 [ 0.]]
v[&quot;dW2&quot;] = [[ 0.  0.  0.]
 [ 0.  0.  0.]
 [ 0.  0.  0.]]
v[&quot;db2&quot;] = [[ 0.]
 [ 0.]
 [ 0.]]</code></pre><p><strong>Expected Output</strong>:</p>
<table style="width:40%"> 
    <tr>
    <td > **v["dW1"]** </td> 
           <td > [[ 0.  0.  0.]
 [ 0.  0.  0.]] </td> 
    </tr> 

<pre><code>&lt;tr&gt;
&lt;td &gt; **v[&quot;db1&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[ 0.]</code></pre><p> [ 0.]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **v[&quot;dW2&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[ 0.  0.  0.]</code></pre><p> [ 0.  0.  0.]<br> [ 0.  0.  0.]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **v[&quot;db2&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[ 0.]</code></pre><p> [ 0.]<br> [ 0.]] </td><br>    </tr> </p>
</table>


<p><strong>Exercise</strong>:  Now, implement the parameters update with momentum. The momentum update rule is, for $l = 1, …, L$: </p>
<p>$$ \begin{cases}<br>v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \<br>W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}}<br>\end{cases}\tag{3}$$</p>
<p>$$\begin{cases}<br>v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \<br>b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}}<br>\end{cases}\tag{4}$$</p>
<p>where L is the number of layers, $\beta$ is the momentum and $\alpha$ is the learning rate. All parameters should be stored in the <code>parameters</code> dictionary.  Note that the iterator <code>l</code> starts at 0 in the <code>for</code> loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that’s a “one” on the superscript). So you will need to shift <code>l</code> to <code>l+1</code> when coding.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters_with_momentum</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_momentum</span><span class="params">(parameters, grads, v, beta, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using Momentum</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity:</span></span><br><span class="line"><span class="string">                    v['dW' + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v['db' + str(l)] = ...</span></span><br><span class="line"><span class="string">    beta -- the momentum hyperparameter, scalar</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- python dictionary containing your updated velocities</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Momentum update for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 4 lines)</span></span><br><span class="line">        <span class="comment"># compute velocities</span></span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = beta*v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]+(<span class="number">1</span>-beta)*grads[<span class="string">'dW'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta*v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]+(<span class="number">1</span>-beta)* grads[<span class="string">'db'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        <span class="comment"># update parameters</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)]-learning_rate*v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] =  parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)]-learning_rate* v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters, v</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads, v = update_parameters_with_momentum_test_case()</span><br><span class="line"></span><br><span class="line">parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = <span class="number">0.9</span>, learning_rate = <span class="number">0.01</span>)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW1\"] = "</span> + str(v[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db1\"] = "</span> + str(v[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW2\"] = "</span> + str(v[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db2\"] = "</span> + str(v[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[ 1.62544598 -0.61290114 -0.52907334]
 [-1.07347112  0.86450677 -2.30085497]]
b1 = [[ 1.74493465]
 [-0.76027113]]
W2 = [[ 0.31930698 -0.24990073  1.4627996 ]
 [-2.05974396 -0.32173003 -0.38320915]
 [ 1.13444069 -1.0998786  -0.1713109 ]]
b2 = [[-0.87809283]
 [ 0.04055394]
 [ 0.58207317]]
v[&quot;dW1&quot;] = [[-0.11006192  0.11447237  0.09015907]
 [ 0.05024943  0.09008559 -0.06837279]]
v[&quot;db1&quot;] = [[-0.01228902]
 [-0.09357694]]
v[&quot;dW2&quot;] = [[-0.02678881  0.05303555 -0.06916608]
 [-0.03967535 -0.06871727 -0.08452056]
 [-0.06712461 -0.00126646 -0.11173103]]
v[&quot;db2&quot;] = [[ 0.02344157]
 [ 0.16598022]
 [ 0.07420442]]</code></pre><p><strong>Expected Output</strong>:</p>
<table style="width:90%"> 
    <tr>
    <td > **W1** </td> 
           <td > [[ 1.62544598 -0.61290114 -0.52907334]
 [-1.07347112  0.86450677 -2.30085497]] </td> 
    </tr> 

<pre><code>&lt;tr&gt;
&lt;td &gt; **b1** &lt;/td&gt; 
       &lt;td &gt; [[ 1.74493465]</code></pre><p> [-0.76027113]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **W2** &lt;/td&gt; 
       &lt;td &gt; [[ 0.31930698 -0.24990073  1.4627996 ]</code></pre><p> [-2.05974396 -0.32173003 -0.38320915]<br> [ 1.13444069 -1.0998786  -0.1713109 ]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **b2** &lt;/td&gt; 
       &lt;td &gt; [[-0.87809283]</code></pre><p> [ 0.04055394]<br> [ 0.58207317]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **v[&quot;dW1&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[-0.11006192  0.11447237  0.09015907]</code></pre><p> [ 0.05024943  0.09008559 -0.06837279]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **v[&quot;db1&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[-0.01228902]</code></pre><p> [-0.09357694]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **v[&quot;dW2&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[-0.02678881  0.05303555 -0.06916608]</code></pre><p> [-0.03967535 -0.06871727 -0.08452056]<br> [-0.06712461 -0.00126646 -0.11173103]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **v[&quot;db2&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[ 0.02344157]</code></pre><p> [ 0.16598022]<br> [ 0.07420442]]</td><br>    </tr> </p>
</table>



<p><strong>Note</strong> that:</p>
<ul>
<li>The velocity is initialized with zeros. So the algorithm will take a few iterations to “build up” velocity and start to take bigger steps.</li>
<li>If $\beta = 0$, then this just becomes standard gradient descent without momentum. </li>
</ul>
<p><strong>How do you choose $\beta$?</strong></p>
<ul>
<li>The larger the momentum $\beta$ is, the smoother the update because the more we take the past gradients into account. But if $\beta$ is too big, it could also smooth out the updates too much. </li>
<li>Common values for $\beta$ range from 0.8 to 0.999. If you don’t feel inclined to tune this, $\beta = 0.9$ is often a reasonable default. </li>
<li>Tuning the optimal $\beta$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$. </li>
</ul>
<font color='blue'>
**What you should remember**:
- Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.
- You have to tune a momentum hyperparameter $\beta$ and a learning rate $\alpha$.

<h2 id="4-Adam"><a href="#4-Adam" class="headerlink" title="4 - Adam"></a>4 - Adam</h2><p>Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. </p>
<p><strong>How does Adam work?</strong></p>
<ol>
<li>It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). </li>
<li>It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). </li>
<li>It updates parameters in a direction based on combining information from “1” and “2”.</li>
</ol>
<p>The update rule is, for $l = 1, …, L$: </p>
<p>$$\begin{cases}<br>v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \<br>v^{corrected}<em>{dW^{[l]}} = \frac{v</em>{dW^{[l]}}}{1 - (\beta_1)^t} \<br>s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \<br>s^{corrected}<em>{dW^{[l]}} = \frac{s</em>{dW^{[l]}}}{1 - (\beta_1)^t} \<br>W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}<em>{dW^{[l]}}}{\sqrt{s^{corrected}</em>{dW^{[l]}}} + \varepsilon}<br>\end{cases}$$<br>where:</p>
<ul>
<li>t counts the number of steps taken of Adam </li>
<li>L is the number of layers</li>
<li>$\beta_1$ and $\beta_2$ are hyperparameters that control the two exponentially weighted averages. </li>
<li>$\alpha$ is the learning rate</li>
<li>$\varepsilon$ is a very small number to avoid dividing by zero</li>
</ul>
<p>As usual, we will store all parameters in the <code>parameters</code> dictionary  </p>
<p><strong>Exercise</strong>: Initialize the Adam variables $v, s$ which keep track of the past information.</p>
<p><strong>Instruction</strong>: The variables $v, s$ are python dictionaries that need to be initialized with arrays of zeros. Their keys are the same as for <code>grads</code>, that is:<br>for $l = 1, …, L$:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["W" + str(l+1)])</span></span><br><span class="line">v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["b" + str(l+1)])</span></span><br><span class="line">s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["W" + str(l+1)])</span></span><br><span class="line">s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["b" + str(l+1)])</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_adam</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_adam</span><span class="params">(parameters)</span> :</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes v and s as two python dictionaries with:</span></span><br><span class="line"><span class="string">                - keys: "dW1", "db1", ..., "dWL", "dbL" </span></span><br><span class="line"><span class="string">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters["W" + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters["b" + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span></span><br><span class="line"><span class="string">                    v["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v["db" + str(l)] = ...</span></span><br><span class="line"><span class="string">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span></span><br><span class="line"><span class="string">                    s["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    s["db" + str(l)] = ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    s = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize v, s. Input: "parameters". Outputs: "v, s".</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)</span></span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)].shape))</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)].shape))</span><br><span class="line">        s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)].shape))</span><br><span class="line">        s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)].shape))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> v, s</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_adam_test_case()</span><br><span class="line"></span><br><span class="line">v, s = initialize_adam(parameters)</span><br><span class="line">print(<span class="string">"v[\"dW1\"] = "</span> + str(v[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db1\"] = "</span> + str(v[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW2\"] = "</span> + str(v[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db2\"] = "</span> + str(v[<span class="string">"db2"</span>]))</span><br><span class="line">print(<span class="string">"s[\"dW1\"] = "</span> + str(s[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"s[\"db1\"] = "</span> + str(s[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"s[\"dW2\"] = "</span> + str(s[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"s[\"db2\"] = "</span> + str(s[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>v[&quot;dW1&quot;] = [[ 0.  0.  0.]
 [ 0.  0.  0.]]
v[&quot;db1&quot;] = [[ 0.]
 [ 0.]]
v[&quot;dW2&quot;] = [[ 0.  0.  0.]
 [ 0.  0.  0.]
 [ 0.  0.  0.]]
v[&quot;db2&quot;] = [[ 0.]
 [ 0.]
 [ 0.]]
s[&quot;dW1&quot;] = [[ 0.  0.  0.]
 [ 0.  0.  0.]]
s[&quot;db1&quot;] = [[ 0.]
 [ 0.]]
s[&quot;dW2&quot;] = [[ 0.  0.  0.]
 [ 0.  0.  0.]
 [ 0.  0.  0.]]
s[&quot;db2&quot;] = [[ 0.]
 [ 0.]
 [ 0.]]</code></pre><p><strong>Expected Output</strong>:</p>
<table style="width:40%"> 
    <tr>
    <td > **v["dW1"]** </td> 
           <td > [[ 0.  0.  0.]
 [ 0.  0.  0.]] </td> 
    </tr> 

<pre><code>&lt;tr&gt;
&lt;td &gt; **v[&quot;db1&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[ 0.]</code></pre><p> [ 0.]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **v[&quot;dW2&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[ 0.  0.  0.]</code></pre><p> [ 0.  0.  0.]<br> [ 0.  0.  0.]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **v[&quot;db2&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[ 0.]</code></pre><p> [ 0.]<br> [ 0.]] </td><br>    </tr><br>    <tr><br>    <td > <strong>s[“dW1”]</strong> </td><br>           <td > [[ 0.  0.  0.]<br> [ 0.  0.  0.]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **s[&quot;db1&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[ 0.]</code></pre><p> [ 0.]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **s[&quot;dW2&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[ 0.  0.  0.]</code></pre><p> [ 0.  0.  0.]<br> [ 0.  0.  0.]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **s[&quot;db2&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[ 0.]</code></pre><p> [ 0.]<br> [ 0.]] </td><br>    </tr></p>
</table>


<p><strong>Exercise</strong>:  Now, implement the parameters update with Adam. Recall the general update rule is, for $l = 1, …, L$: </p>
<p>$$\begin{cases}<br>v_{W^{[l]}} = \beta_1 v_{W^{[l]}} + (1 - \beta_1) \frac{\partial J }{ \partial W^{[l]} } \<br>v^{corrected}<em>{W^{[l]}} = \frac{v</em>{W^{[l]}}}{1 - (\beta_1)^t} \<br>s_{W^{[l]}} = \beta_2 s_{W^{[l]}} + (1 - \beta_2) (\frac{\partial J }{\partial W^{[l]} })^2 \<br>s^{corrected}<em>{W^{[l]}} = \frac{s</em>{W^{[l]}}}{1 - (\beta_2)^t} \<br>W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}<em>{W^{[l]}}}{\sqrt{s^{corrected}</em>{W^{[l]}}}+\varepsilon}<br>\end{cases}$$</p>
<p><strong>Note</strong> that the iterator <code>l</code> starts at 0 in the <code>for</code> loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift <code>l</code> to <code>l+1</code> when coding.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters_with_adam</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_adam</span><span class="params">(parameters, grads, v, s, t, learning_rate = <span class="number">0.01</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                                beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>,  epsilon = <span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using Adam</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></span><br><span class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span>                 <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    v_corrected = &#123;&#125;                         <span class="comment"># Initializing first moment estimate, python dictionary</span></span><br><span class="line">    s_corrected = &#123;&#125;                         <span class="comment"># Initializing second moment estimate, python dictionary</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Perform Adam update on all parameters</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        <span class="comment"># Moving average of the gradients. Inputs: "v, grads, beta1". Output: "v".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = beta1*v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]+(<span class="number">1</span>-beta1)*grads[<span class="string">'dW'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta1*v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]+(<span class="number">1</span>-beta1)*grads[<span class="string">'db'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute bias-corrected first moment estimate. Inputs: "v, beta1, t". Output: "v_corrected".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        v_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] =  v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]/(<span class="number">1</span>-beta1**t)</span><br><span class="line">        v_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] =  v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]/(<span class="number">1</span>-beta1**t)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Moving average of the squared gradients. Inputs: "s, grads, beta2". Output: "s".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = beta2*s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]+(<span class="number">1</span>-beta2)*np.multiply(grads[<span class="string">'dW'</span> + str(l+<span class="number">1</span>)],grads[<span class="string">'dW'</span> + str(l+<span class="number">1</span>)])</span><br><span class="line">        s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta2*s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]+(<span class="number">1</span>-beta2)*np.multiply(grads[<span class="string">'db'</span> + str(l+<span class="number">1</span>)],grads[<span class="string">'db'</span> + str(l+<span class="number">1</span>)])</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute bias-corrected second raw moment estimate. Inputs: "s, beta2, t". Output: "s_corrected".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        s_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]/(<span class="number">1</span>-beta2**t)</span><br><span class="line">        s_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]/(<span class="number">1</span>-beta2**t)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters. Inputs: "parameters, learning_rate, v_corrected, s_corrected, epsilon". Output: "parameters".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)]-learning_rate*v_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]/np.sqrt( s_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]+epsilon)</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)]-learning_rate*v_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]/np.sqrt(s_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]+epsilon)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters, v, s</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads, v, s = update_parameters_with_adam_test_case()</span><br><span class="line">parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW1\"] = "</span> + str(v[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db1\"] = "</span> + str(v[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW2\"] = "</span> + str(v[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db2\"] = "</span> + str(v[<span class="string">"db2"</span>]))</span><br><span class="line">print(<span class="string">"s[\"dW1\"] = "</span> + str(s[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"s[\"db1\"] = "</span> + str(s[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"s[\"dW2\"] = "</span> + str(s[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"s[\"db2\"] = "</span> + str(s[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[ 1.63178673 -0.61919778 -0.53561312]
 [-1.08040999  0.85796626 -2.29409733]]
b1 = [[ 1.75225313]
 [-0.75376553]]
W2 = [[ 0.32648046 -0.25681174  1.46954931]
 [-2.05269934 -0.31497584 -0.37661299]
 [ 1.14121081 -1.09245036 -0.16498684]]
b2 = [[-0.88529978]
 [ 0.03477238]
 [ 0.57537385]]
v[&quot;dW1&quot;] = [[-0.11006192  0.11447237  0.09015907]
 [ 0.05024943  0.09008559 -0.06837279]]
v[&quot;db1&quot;] = [[-0.01228902]
 [-0.09357694]]
v[&quot;dW2&quot;] = [[-0.02678881  0.05303555 -0.06916608]
 [-0.03967535 -0.06871727 -0.08452056]
 [-0.06712461 -0.00126646 -0.11173103]]
v[&quot;db2&quot;] = [[ 0.02344157]
 [ 0.16598022]
 [ 0.07420442]]
s[&quot;dW1&quot;] = [[ 0.00121136  0.00131039  0.00081287]
 [ 0.0002525   0.00081154  0.00046748]]
s[&quot;db1&quot;] = [[  1.51020075e-05]
 [  8.75664434e-04]]
s[&quot;dW2&quot;] = [[  7.17640232e-05   2.81276921e-04   4.78394595e-04]
 [  1.57413361e-04   4.72206320e-04   7.14372576e-04]
 [  4.50571368e-04   1.60392066e-07   1.24838242e-03]]
s[&quot;db2&quot;] = [[  5.49507194e-05]
 [  2.75494327e-03]
 [  5.50629536e-04]]</code></pre><p><strong>Expected Output</strong>:</p>
<table> 
    <tr>
    <td > **W1** </td> 
           <td > [[ 1.63178673 -0.61919778 -0.53561312]
 [-1.08040999  0.85796626 -2.29409733]] </td> 
    </tr> 

<pre><code>&lt;tr&gt;
&lt;td &gt; **b1** &lt;/td&gt; 
       &lt;td &gt; [[ 1.75225313]</code></pre><p> [-0.75376553]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **W2** &lt;/td&gt; 
       &lt;td &gt; [[ 0.32648046 -0.25681174  1.46954931]</code></pre><p> [-2.05269934 -0.31497584 -0.37661299]<br> [ 1.14121081 -1.09245036 -0.16498684]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **b2** &lt;/td&gt; 
       &lt;td &gt; [[-0.88529978]</code></pre><p> [ 0.03477238]<br> [ 0.57537385]] </td><br>    </tr><br>    <tr><br>    <td > <strong>v[“dW1”]</strong> </td><br>           <td > [[-0.11006192  0.11447237  0.09015907]<br> [ 0.05024943  0.09008559 -0.06837279]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **v[&quot;db1&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[-0.01228902]</code></pre><p> [-0.09357694]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **v[&quot;dW2&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[-0.02678881  0.05303555 -0.06916608]</code></pre><p> [-0.03967535 -0.06871727 -0.08452056]<br> [-0.06712461 -0.00126646 -0.11173103]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **v[&quot;db2&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[ 0.02344157]</code></pre><p> [ 0.16598022]<br> [ 0.07420442]] </td><br>    </tr><br>    <tr><br>    <td > <strong>s[“dW1”]</strong> </td><br>           <td > [[ 0.00121136  0.00131039  0.00081287]<br> [ 0.0002525   0.00081154  0.00046748]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **s[&quot;db1&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[  1.51020075e-05]</code></pre><p> [  8.75664434e-04]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **s[&quot;dW2&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[  7.17640232e-05   2.81276921e-04   4.78394595e-04]</code></pre><p> [  1.57413361e-04   4.72206320e-04   7.14372576e-04]<br> [  4.50571368e-04   1.60392066e-07   1.24838242e-03]] </td><br>    </tr> </p>
<pre><code>&lt;tr&gt;
&lt;td &gt; **s[&quot;db2&quot;]** &lt;/td&gt; 
       &lt;td &gt; [[  5.49507194e-05]</code></pre><p> [  2.75494327e-03]<br> [  5.50629536e-04]] </td><br>    </tr></p>
</table>


<p>You now have three working optimization algorithms (mini-batch gradient descent, Momentum, Adam). Let’s implement a model with each of these optimizers and observe the difference.</p>
<h2 id="5-Model-with-different-optimization-algorithms"><a href="#5-Model-with-different-optimization-algorithms" class="headerlink" title="5 - Model with different optimization algorithms"></a>5 - Model with different optimization algorithms</h2><p>Lets use the following “moons” dataset to test the different optimization methods. (The dataset is named “moons” because the data from each of the two classes looks a bit like a crescent-shaped moon.) </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y = load_dataset()</span><br></pre></td></tr></table></figure>


<p><img src="output_34_0.png" alt="png"></p>
<p>We have already implemented a 3-layer neural network. You will train it with: </p>
<ul>
<li>Mini-batch <strong>Gradient Descent</strong>: it will call your function:<ul>
<li><code>update_parameters_with_gd()</code></li>
</ul>
</li>
<li>Mini-batch <strong>Momentum</strong>: it will call your functions:<ul>
<li><code>initialize_velocity()</code> and <code>update_parameters_with_momentum()</code></li>
</ul>
</li>
<li>Mini-batch <strong>Adam</strong>: it will call your functions:<ul>
<li><code>initialize_adam()</code> and <code>update_parameters_with_adam()</code></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, layers_dims, optimizer, learning_rate = <span class="number">0.0007</span>, mini_batch_size = <span class="number">64</span>, beta = <span class="number">0.9</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">          beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>,  epsilon = <span class="number">1e-8</span>, num_epochs = <span class="number">10000</span>, print_cost = True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    3-layer neural network model which can be run in different optimizer modes.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- python list, containing the size of each layer</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    mini_batch_size -- the size of a mini batch</span></span><br><span class="line"><span class="string">    beta -- Momentum hyperparameter</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the past gradients estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates </span></span><br><span class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 1000 epochs</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(layers_dims)             <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    costs = []                       <span class="comment"># to keep track of the cost</span></span><br><span class="line">    t = <span class="number">0</span>                            <span class="comment"># initializing the counter required for Adam update</span></span><br><span class="line">    seed = <span class="number">10</span>                        <span class="comment"># For grading purposes, so that your "random" minibatches are the same as ours</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the optimizer</span></span><br><span class="line">    <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># no initialization required for gradient descent</span></span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">        v = initialize_velocity(parameters)</span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">        v, s = initialize_adam(parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span></span><br><span class="line">        seed = seed + <span class="number">1</span></span><br><span class="line">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Select a minibatch</span></span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward propagation</span></span><br><span class="line">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute cost</span></span><br><span class="line">            cost = compute_cost(a3, minibatch_Y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backward propagation</span></span><br><span class="line">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update parameters</span></span><br><span class="line">            <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">                t = t + <span class="number">1</span> <span class="comment"># Adam counter</span></span><br><span class="line">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,</span><br><span class="line">                                                               t, learning_rate, beta1, beta2,  epsilon)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 epoch</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">                </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'epochs (per 100)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

<p>You will now run this 3 layer neural network with each of the 3 optimization methods.</p>
<h3 id="5-1-Mini-batch-Gradient-descent"><a href="#5-1-Mini-batch-Gradient-descent" class="headerlink" title="5.1 - Mini-batch Gradient descent"></a>5.1 - Mini-batch Gradient descent</h3><p>Run the following code to see how the model does with mini-batch gradient descent.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"gd"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Gradient Descent optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>

<pre><code>Cost after epoch 0: 0.690736
Cost after epoch 1000: 0.685273
Cost after epoch 2000: 0.647072
Cost after epoch 3000: 0.619525
Cost after epoch 4000: 0.576584
Cost after epoch 5000: 0.607243
Cost after epoch 6000: 0.529403
Cost after epoch 7000: 0.460768
Cost after epoch 8000: 0.465586
Cost after epoch 9000: 0.464518</code></pre><p><img src="output_38_1.png" alt="png"></p>
<pre><code>Accuracy: 0.796666666667</code></pre><p><img src="output_38_3.png" alt="png"></p>
<h3 id="5-2-Mini-batch-gradient-descent-with-momentum"><a href="#5-2-Mini-batch-gradient-descent-with-momentum" class="headerlink" title="5.2 - Mini-batch gradient descent with momentum"></a>5.2 - Mini-batch gradient descent with momentum</h3><p>Run the following code to see how the model does with momentum. Because this example is relatively simple, the gains from using momemtum are small; but for more complex problems you might see bigger gains.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, beta = <span class="number">0.9</span>, optimizer = <span class="string">"momentum"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Momentum optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>

<pre><code>Cost after epoch 0: 0.690741
Cost after epoch 1000: 0.685341
Cost after epoch 2000: 0.647145
Cost after epoch 3000: 0.619594
Cost after epoch 4000: 0.576665
Cost after epoch 5000: 0.607324
Cost after epoch 6000: 0.529476
Cost after epoch 7000: 0.460936
Cost after epoch 8000: 0.465780
Cost after epoch 9000: 0.464740</code></pre><p><img src="output_40_1.png" alt="png"></p>
<pre><code>Accuracy: 0.796666666667</code></pre><p><img src="output_40_3.png" alt="png"></p>
<h3 id="5-3-Mini-batch-with-Adam-mode"><a href="#5-3-Mini-batch-with-Adam-mode" class="headerlink" title="5.3 - Mini-batch with Adam mode"></a>5.3 - Mini-batch with Adam mode</h3><p>Run the following code to see how the model does with Adam.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"adam"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Adam optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>

<pre><code>Cost after epoch 0: 0.690552
Cost after epoch 1000: 0.185501
Cost after epoch 2000: 0.150830
Cost after epoch 3000: 0.074454
Cost after epoch 4000: 0.125959
Cost after epoch 5000: 0.104344
Cost after epoch 6000: 0.100676
Cost after epoch 7000: 0.031652
Cost after epoch 8000: 0.111973
Cost after epoch 9000: 0.197940</code></pre><p><img src="output_42_1.png" alt="png"></p>
<pre><code>Accuracy: 0.94</code></pre><p><img src="output_42_3.png" alt="png"></p>
<h3 id="5-4-Summary"><a href="#5-4-Summary" class="headerlink" title="5.4 - Summary"></a>5.4 - Summary</h3><table> 
    <tr>
        <td>
        **optimization method**
        </td>
        <td>
        **accuracy**
        </td>
        <td>
        **cost shape**
        </td>

<pre><code>&lt;/tr&gt;
    &lt;td&gt;
    Gradient descent
    &lt;/td&gt;
    &lt;td&gt;
    79.7%
    &lt;/td&gt;
    &lt;td&gt;
    oscillations
    &lt;/td&gt;
&lt;tr&gt;
    &lt;td&gt;
    Momentum
    &lt;/td&gt;
    &lt;td&gt;
    79.7%
    &lt;/td&gt;
    &lt;td&gt;
    oscillations
    &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
    &lt;td&gt;
    Adam
    &lt;/td&gt;
    &lt;td&gt;
    94%
    &lt;/td&gt;
    &lt;td&gt;
    smoother
    &lt;/td&gt;
&lt;/tr&gt;</code></pre></table> 

<p>Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.</p>
<p>Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster.</p>
<p>Some advantages of Adam include:</p>
<ul>
<li>Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) </li>
<li>Usually works well even with little tuning of hyperparameters (except $\alpha$)</li>
</ul>
<p><strong>References</strong>:</p>
<ul>
<li>Adam paper: <a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1412.6980.pdf</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/17/GD/" data-id="ck7t8treh0003k6s60ixphd3y" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/" rel="tag">deep learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-coursera练习——神经网络的建立1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/10/coursera%E7%BB%83%E4%B9%A0%E2%80%94%E2%80%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BB%BA%E7%AB%8B1/" class="article-date">
  <time datetime="2018-12-10T11:40:21.000Z" itemprop="datePublished">2018-12-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/10/coursera%E7%BB%83%E4%B9%A0%E2%80%94%E2%80%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BB%BA%E7%AB%8B1/">coursera练习——神经网络的建立1</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Planar-data-classification-with-one-hidden-layer"><a href="#Planar-data-classification-with-one-hidden-layer" class="headerlink" title="Planar data classification with one hidden layer"></a>Planar data classification with one hidden layer</h1><p>Welcome to your week 3 programming assignment. It’s time to build your first neural network, which will have a hidden layer. You will see a big difference between this model and the one you implemented using logistic regression. </p>
<p><strong>You will learn how to:</strong></p>
<ul>
<li>Implement a 2-class classification neural network with a single hidden layer</li>
<li>Use units with a non-linear activation function, such as tanh </li>
<li>Compute the cross entropy loss </li>
<li>Implement forward and backward propagation</li>
</ul>
<h2 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1 - Packages"></a>1 - Packages</h2><p>Let’s first import all the packages that you will need during this assignment.</p>
<ul>
<li><a href="www.numpy.org">numpy</a> is the fundamental package for scientific computing with Python.</li>
<li><a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">sklearn</a> provides simple and efficient tools for data mining and data analysis. </li>
<li><a href="http://matplotlib.org" target="_blank" rel="noopener">matplotlib</a> is a library for plotting graphs in Python.</li>
<li>testCases provides some test examples to assess the correctness of your functions</li>
<li>planar_utils provide various useful functions used in this assignment</li>
</ul>
<h1 id="Package-imports"><a href="#Package-imports" class="headerlink" title="Package imports"></a>Package imports</h1><p>import numpy as np<br>import matplotlib.pyplot as plt<br>from testCases_v2 import *<br>import sklearn<br>import sklearn.datasets<br>import sklearn.linear_model<br>from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</p>
<p>%matplotlib inline</p>
<p>np.random.seed(1) # set a seed so that the results are consistent</p>
<h2 id="2-Dataset"><a href="#2-Dataset" class="headerlink" title="2 - Dataset"></a>2 - Dataset</h2><p>First, let’s get the dataset you will work on. The following code will load a “flower” 2-class dataset into variables <code>X</code> and <code>Y</code>.</p>
<p>X, Y = load_planar_dataset()</p>
<p>Visualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Visualize the data:</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y, s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br></pre></td></tr></table></figure>


<p><img src="output_6_0.png" alt="png"></p>
<p>You have:<br>    - a numpy-array (matrix) X that contains your features (x1, x2)<br>    - a numpy-array (vector) Y that contains your labels (red:0, blue:1).</p>
<p>Lets first get a better sense of what our data is like. </p>
<p><strong>Exercise</strong>: How many training examples do you have? In addition, what is the <code>shape</code> of the variables <code>X</code> and <code>Y</code>? </p>
<p><strong>Hint</strong>: How do you get the shape of a numpy array? <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html" target="_blank" rel="noopener">(help)</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">shape_X = X.shape</span><br><span class="line">shape_Y = Y.shape</span><br><span class="line">m = shape_X[<span class="number">1</span>] <span class="comment"># training set size</span></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">'The shape of X is: '</span> + str(shape_X))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'The shape of Y is: '</span> + str(shape_Y))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'I have m = %d training examples!'</span> % (m))</span><br></pre></td></tr></table></figure>

<pre><code>The shape of X is: (2, 200)
The shape of Y is: (1, 200)
I have m = 200 training examples!</code></pre><p><strong>Expected Output</strong>:</p>
<table style="width:20%">

  <tr>
    <td>**shape of X**</td>
    <td> (2, 400) </td> 
  </tr>

  <tr>
    <td>**shape of Y**</td>
    <td>(1, 400) </td> 
  </tr>

<pre><code>&lt;tr&gt;
&lt;td&gt;**m**&lt;/td&gt;
&lt;td&gt; 400 &lt;/td&gt; </code></pre>  </tr>

</table>

<h2 id="3-Simple-Logistic-Regression"><a href="#3-Simple-Logistic-Regression" class="headerlink" title="3 - Simple Logistic Regression"></a>3 - Simple Logistic Regression</h2><p>Before building a full neural network, lets first see how logistic regression performs on this problem. You can use sklearn’s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the logistic regression classifier</span></span><br><span class="line">clf = sklearn.linear_model.LogisticRegressionCV();</span><br><span class="line">clf.fit(X.T, Y.T);</span><br></pre></td></tr></table></figure>

<pre><code>/opt/conda/lib/python3.5/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)</code></pre><p>You can now plot the decision boundary of these models. Run the code below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the decision boundary for logistic regression</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: clf.predict(x), X, Y)</span><br><span class="line">plt.title(<span class="string">"Logistic Regression"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">LR_predictions = clf.predict(X.T)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy of logistic regression: %d '</span> % float((np.dot(Y,LR_predictions) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-LR_predictions))/float(Y.size)*<span class="number">100</span>) +</span><br><span class="line">       <span class="string">'% '</span> + <span class="string">"(percentage of correctly labelled datapoints)"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)</code></pre><p><img src="output_13_1.png" alt="png"></p>
<p><strong>Expected Output</strong>:</p>
<table style="width:20%">
  <tr>
    <td>**Accuracy**</td>
    <td> 47% </td> 
  </tr>

</table>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">**Interpretation**: The dataset <span class="keyword">is</span> <span class="keyword">not</span> linearly separable, so logistic regression doesn<span class="string">'t perform well. Hopefully a neural network will do better. Let'</span>s <span class="keyword">try</span> this now!</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4 - Neural Network model</span></span><br><span class="line"></span><br><span class="line">Logistic regression did <span class="keyword">not</span> work well on the <span class="string">"flower dataset"</span>. You are going to train a Neural Network <span class="keyword">with</span> a single hidden layer.</span><br><span class="line"></span><br><span class="line">**Here <span class="keyword">is</span> our model**:</span><br><span class="line">&lt;img src=<span class="string">"images/classification_kiank.png"</span> style=<span class="string">"width:600px;height:300px;"</span>&gt;</span><br><span class="line"></span><br><span class="line">**Mathematically**:</span><br><span class="line"></span><br><span class="line">For one example $x^&#123;(i)&#125;$:</span><br><span class="line">$$z^&#123;[<span class="number">1</span>] (i)&#125; =  W^&#123;[<span class="number">1</span>]&#125; x^&#123;(i)&#125; + b^&#123;[<span class="number">1</span>]&#125;\tag&#123;<span class="number">1</span>&#125;$$ </span><br><span class="line">$$a^&#123;[<span class="number">1</span>] (i)&#125; = \tanh(z^&#123;[<span class="number">1</span>] (i)&#125;)\tag&#123;<span class="number">2</span>&#125;$$</span><br><span class="line">$$z^&#123;[<span class="number">2</span>] (i)&#125; = W^&#123;[<span class="number">2</span>]&#125; a^&#123;[<span class="number">1</span>] (i)&#125; + b^&#123;[<span class="number">2</span>]&#125;\tag&#123;<span class="number">3</span>&#125;$$</span><br><span class="line">$$\hat&#123;y&#125;^&#123;(i)&#125; = a^&#123;[<span class="number">2</span>] (i)&#125; = \sigma(z^&#123; [<span class="number">2</span>] (i)&#125;)\tag&#123;<span class="number">4</span>&#125;$$</span><br><span class="line">$$y^&#123;(i)&#125;_&#123;prediction&#125; = \begin&#123;cases&#125; <span class="number">1</span> &amp; \mbox&#123;<span class="keyword">if</span> &#125; a^&#123;[<span class="number">2</span>](i)&#125; &gt; <span class="number">0.5</span> \\ <span class="number">0</span> &amp; \mbox&#123;otherwise &#125; \end&#123;cases&#125;\tag&#123;<span class="number">5</span>&#125;$$</span><br><span class="line"></span><br><span class="line">Given the predictions on all the examples, you can also compute the cost $J$ <span class="keyword">as</span> follows: </span><br><span class="line">$$J = - \frac&#123;<span class="number">1</span>&#125;&#123;m&#125; \sum\limits_&#123;i = <span class="number">0</span>&#125;^&#123;m&#125; \large\left(\small y^&#123;(i)&#125;\log\left(a^&#123;[<span class="number">2</span>] (i)&#125;\right) + (<span class="number">1</span>-y^&#123;(i)&#125;)\log\left(<span class="number">1</span>- a^&#123;[<span class="number">2</span>] (i)&#125;\right)  \large  \right) \small \tag&#123;<span class="number">6</span>&#125;$$</span><br><span class="line"></span><br><span class="line">**Reminder**: The general methodology to build a Neural Network <span class="keyword">is</span> to:</span><br><span class="line">    <span class="number">1.</span> Define the neural network structure ( <span class="comment"># of input units,  # of hidden units, etc). </span></span><br><span class="line">    <span class="number">2.</span> Initialize the model<span class="string">'s parameters</span></span><br><span class="line"><span class="string">    3. Loop:</span></span><br><span class="line"><span class="string">        - Implement forward propagation</span></span><br><span class="line"><span class="string">        - Compute loss</span></span><br><span class="line"><span class="string">        - Implement backward propagation to get the gradients</span></span><br><span class="line"><span class="string">        - Update parameters (gradient descent)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">You often build helper functions to compute steps 1-3 and then merge them into one function we call `nn_model()`. Once you'</span>ve built `nn_model()` <span class="keyword">and</span> learnt the right parameters, you can make predictions on new data.</span><br></pre></td></tr></table></figure>

<h3 id="4-1-Defining-the-neural-network-structure"><a href="#4-1-Defining-the-neural-network-structure" class="headerlink" title="4.1 - Defining the neural network structure"></a>4.1 - Defining the neural network structure</h3><p><strong>Exercise</strong>: Define three variables:<br>    - n_x: the size of the input layer<br>    - n_h: the size of the hidden layer (set this to 4)<br>    - n_y: the size of the output layer</p>
<p><strong>Hint</strong>: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: layer_sizes</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    n_x -- the size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- the size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- the size of the output layer</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer</span></span><br><span class="line">    n_h = <span class="number">4</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># size of output layer</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess = layer_sizes_test_case()</span><br><span class="line">(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)</span><br><span class="line">print(<span class="string">"The size of the input layer is: n_x = "</span> + str(n_x))</span><br><span class="line">print(<span class="string">"The size of the hidden layer is: n_h = "</span> + str(n_h))</span><br><span class="line">print(<span class="string">"The size of the output layer is: n_y = "</span> + str(n_y))</span><br></pre></td></tr></table></figure>

<pre><code>The size of the input layer is: n_x = 5
The size of the hidden layer is: n_h = 4
The size of the output layer is: n_y = 2</code></pre><p><strong>Expected Output</strong> (these are not the sizes you will use for your network, they are just used to assess the function you’ve just coded).</p>
<table style="width:20%">
  <tr>
    <td>**n_x**</td>
    <td> 5 </td> 
  </tr>

<pre><code>&lt;tr&gt;
&lt;td&gt;**n_h**&lt;/td&gt;
&lt;td&gt; 4 &lt;/td&gt; </code></pre>  </tr>

<pre><code>&lt;tr&gt;
&lt;td&gt;**n_y**&lt;/td&gt;
&lt;td&gt; 2 &lt;/td&gt; </code></pre>  </tr>

</table>

<h3 id="4-2-Initialize-the-model’s-parameters"><a href="#4-2-Initialize-the-model’s-parameters" class="headerlink" title="4.2 - Initialize the model’s parameters"></a>4.2 - Initialize the model’s parameters</h3><p><strong>Exercise</strong>: Implement the function <code>initialize_parameters()</code>.</p>
<p><strong>Instructions</strong>:</p>
<ul>
<li>Make sure your parameters’ sizes are right. Refer to the neural network figure above if needed.</li>
<li>You will initialize the weights matrices with random values. <ul>
<li>Use: <code>np.random.randn(a,b) * 0.01</code> to randomly initialize a matrix of shape (a,b).</li>
</ul>
</li>
<li>You will initialize the bias vectors as zeros. <ul>
<li>Use: <code>np.zeros((a,b))</code> to initialize a matrix of shape (a,b) with zeros.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">2</span>) <span class="comment"># we set up a seed so that your output matches ours although the initialization is random.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n_x, n_h, n_y = initialize_parameters_test_case()</span><br><span class="line"></span><br><span class="line">parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[-0.00416758 -0.00056267]
 [-0.02136196  0.01640271]
 [-0.01793436 -0.00841747]
 [ 0.00502881 -0.01245288]]
b1 = [[ 0.]
 [ 0.]
 [ 0.]
 [ 0.]]
W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]
b2 = [[ 0.]]</code></pre><p><strong>Expected Output</strong>:</p>
<table style="width:90%">
  <tr>
    <td>**W1**</td>
    <td> [[-0.00416758 -0.00056267]
 [-0.02136196  0.01640271]
 [-0.01793436 -0.00841747]
 [ 0.00502881 -0.01245288]] </td> 
  </tr>

  <tr>
    <td>**b1**</td>
    <td> [[ 0.]
 [ 0.]
 [ 0.]
 [ 0.]] </td> 
  </tr>

  <tr>
    <td>**W2**</td>
    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td> 
  </tr>


  <tr>
    <td>**b2**</td>
    <td> [[ 0.]] </td> 
  </tr>

</table>



<h3 id="4-3-The-Loop"><a href="#4-3-The-Loop" class="headerlink" title="4.3 - The Loop"></a>4.3 - The Loop</h3><p><strong>Question</strong>: Implement <code>forward_propagation()</code>.</p>
<p><strong>Instructions</strong>:</p>
<ul>
<li>Look above at the mathematical representation of your classifier.</li>
<li>You can use the function <code>sigmoid()</code>. It is built-in (imported) in the notebook.</li>
<li>You can use the function <code>np.tanh()</code>. It is part of the numpy library.</li>
<li>The steps you have to implement are:<ol>
<li>Retrieve each parameter from the dictionary “parameters” (which is the output of <code>initialize_parameters()</code>) by using <code>parameters[&quot;..&quot;]</code>.</li>
<li>Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).</li>
</ol>
</li>
<li>Values needed in the backpropagation are stored in “<code>cache</code>“. The <code>cache</code> will be given as an input to the backpropagation function.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: forward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters (output of initialization function)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation</span></span><br><span class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    Z1 = np.dot(W1,X)+b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2,A1)+b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_assess, parameters = forward_propagation_test_case()</span><br><span class="line">A2, cache = forward_propagation(X_assess, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: we use the mean here just to make sure that your output matches ours. </span></span><br><span class="line">print(np.mean(cache[<span class="string">'Z1'</span>]) ,np.mean(cache[<span class="string">'A1'</span>]),np.mean(cache[<span class="string">'Z2'</span>]),np.mean(cache[<span class="string">'A2'</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>0.262818640198 0.091999045227 -1.30766601287 0.212877681719</code></pre><p><strong>Expected Output</strong>:</p>
<table style="width:50%">
  <tr>
    <td> 0.262818640198 0.091999045227 -1.30766601287 0.212877681719 </td> 
  </tr>
</table>

<p>Now that you have computed $A^{[2]}$ (in the Python variable “<code>A2</code>“), which contains $a^{<a href="i">2</a>}$ for every example, you can compute the cost function as follows:</p>
<p>$$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small\tag{13}$$</p>
<p><strong>Exercise</strong>: Implement <code>compute_cost()</code> to compute the value of the cost $J$.</p>
<p><strong>Instructions</strong>:</p>
<ul>
<li>There are many ways to implement the cross-entropy loss. To help you, we give you how we would have implemented<br>$- \sum\limits_{i=0}^{m}  y^{(i)}\log(a^{<a href="i">2</a>})$:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">logprobs = np.multiply(np.log(A2),Y)</span><br><span class="line">cost = - np.sum(logprobs)                <span class="comment"># no need to use a for loop!</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>(you can use either <code>np.multiply()</code> and then <code>np.sum()</code> or directly <code>np.dot()</code>).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cross-entropy cost given in equation (13)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters W1, b1, W2 and b2</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost given equation (13)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2),Y)+np.multiply(np.log(<span class="number">1</span>-A2),(<span class="number">1</span>-Y))</span><br><span class="line">    cost = <span class="number">-1</span>/m*np.sum(logprobs)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect. </span></span><br><span class="line">                                <span class="comment"># E.g., turns [[17]] into 17 </span></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost, float))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A2, Y_assess, parameters = compute_cost_test_case()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"cost = "</span> + str(compute_cost(A2, Y_assess, parameters)))</span><br></pre></td></tr></table></figure>

<pre><code>cost = 0.693058761039</code></pre><p><strong>Expected Output</strong>:</p>
<table style="width:20%">
  <tr>
    <td>**cost**</td>
    <td> 0.693058761... </td> 
  </tr>

</table>

<p>Using the cache computed during forward propagation, you can now implement backward propagation.</p>
<p><strong>Question</strong>: Implement the function <code>backward_propagation()</code>.</p>
<p><strong>Instructions</strong>:<br>Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.  </p>
<img src="images/grad_summary.png" style="width:600px;height:300px;">

<!--
$\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } = \frac{1}{m} (a^{[2](i)} - y^{(i)})$

$\frac{\partial \mathcal{J} }{ \partial W_2 } = \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } a^{[1] (i) T} $

$\frac{\partial \mathcal{J} }{ \partial b_2 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)}}}$

$\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} } =  W_2^T \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $

$\frac{\partial \mathcal{J} }{ \partial W_1 } = \frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} }  X^T $

$\frac{\partial \mathcal{J} _i }{ \partial b_1 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)}}}$

- Note that $*$ denotes elementwise multiplication.
- The notation you will use is common in deep learning coding:
    - dW1 = $\frac{\partial \mathcal{J} }{ \partial W_1 }$
    - db1 = $\frac{\partial \mathcal{J} }{ \partial b_1 }$
    - dW2 = $\frac{\partial \mathcal{J} }{ \partial W_2 }$
    - db2 = $\frac{\partial \mathcal{J} }{ \partial b_2 }$

!-->

<ul>
<li>Tips:<ul>
<li>To compute dZ1 you’ll need to compute $g^{[1]’}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]’}(z) = 1-a^2$. So you can compute<br>$g^{[1]’}(Z^{[1]})$ using <code>(1 - np.power(A1, 2))</code>.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: backward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation using the instructions above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing our parameters </span></span><br><span class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".</span></span><br><span class="line"><span class="string">    X -- input data of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients with respect to different parameters</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary "parameters".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Retrieve also A1 and A2 from dictionary "cache".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)</span></span><br><span class="line">    dZ2 = A2-Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m*np.dot(dZ2,A1.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m*np.sum(dZ2,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.dot(W2.T,dZ2)*(<span class="number">1</span>-np.power(A1,<span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span>/m*np.dot(dZ1,X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m*np.sum(dZ1,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, cache, X_assess, Y_assess = backward_propagation_test_case()</span><br><span class="line"></span><br><span class="line">grads = backward_propagation(parameters, cache, X_assess, Y_assess)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW1 = "</span>+ str(grads[<span class="string">"dW1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db1 = "</span>+ str(grads[<span class="string">"db1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW2 = "</span>+ str(grads[<span class="string">"dW2"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db2 = "</span>+ str(grads[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>dW1 = [[ 0.00301023 -0.00747267]
 [ 0.00257968 -0.00641288]
 [-0.00156892  0.003893  ]
 [-0.00652037  0.01618243]]
db1 = [[ 0.00176201]
 [ 0.00150995]
 [-0.00091736]
 [-0.00381422]]
dW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]
db2 = [[-0.16655712]]</code></pre><p><strong>Expected output</strong>:</p>
<table style="width:80%">
  <tr>
    <td>**dW1**</td>
    <td> [[ 0.00301023 -0.00747267]
 [ 0.00257968 -0.00641288]
 [-0.00156892  0.003893  ]
 [-0.00652037  0.01618243]] </td> 
  </tr>

  <tr>
    <td>**db1**</td>
    <td>  [[ 0.00176201]
 [ 0.00150995]
 [-0.00091736]
 [-0.00381422]] </td> 
  </tr>

  <tr>
    <td>**dW2**</td>
    <td> [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]] </td> 
  </tr>


  <tr>
    <td>**db2**</td>
    <td> [[-0.16655712]] </td> 
  </tr>

</table>  

<p><strong>Question</strong>: Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).</p>
<p><strong>General gradient descent rule</strong>: $ \theta = \theta - \alpha \frac{\partial J }{ \partial \theta }$ where $\alpha$ is the learning rate and $\theta$ represents a parameter.</p>
<p><strong>Illustration</strong>: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.</p>
<p><img src="images/sgd.gif" style="width:400;height:400;"> <img src="images/sgd_bad.gif" style="width:400;height:400;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Updates parameters using the gradient descent update rule given above</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve each gradient from the dictionary "grads"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line">    <span class="comment">## END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = W1-learning_rate*dW1</span><br><span class="line">    b1 = b1-learning_rate*db1</span><br><span class="line">    W2 = W2-learning_rate*dW2</span><br><span class="line">    b2 = b2-learning_rate*db2</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads = update_parameters_test_case()</span><br><span class="line">parameters = update_parameters(parameters, grads)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[-0.00643025  0.01936718]
 [-0.02410458  0.03978052]
 [-0.01653973 -0.02096177]
 [ 0.01046864 -0.05990141]]
b1 = [[ -1.02420756e-06]
 [  1.27373948e-05]
 [  8.32996807e-07]
 [ -3.20136836e-06]]
W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]
b2 = [[ 0.00010457]]</code></pre><p><strong>Expected Output</strong>:</p>
<table style="width:80%">
  <tr>
    <td>**W1**</td>
    <td> [[-0.00643025  0.01936718]
 [-0.02410458  0.03978052]
 [-0.01653973 -0.02096177]
 [ 0.01046864 -0.05990141]]</td> 
  </tr>

  <tr>
    <td>**b1**</td>
    <td> [[ -1.02420756e-06]
 [  1.27373948e-05]
 [  8.32996807e-07]
 [ -3.20136836e-06]]</td> 
  </tr>

  <tr>
    <td>**W2**</td>
    <td> [[-0.01041081 -0.04463285  0.01758031  0.04747113]] </td> 
  </tr>


  <tr>
    <td>**b2**</td>
    <td> [[ 0.00010457]] </td> 
  </tr>

</table>  


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 4.4 - Integrate parts 4.1, 4.2 and 4.3 in nn_model() ####</span></span><br><span class="line"></span><br><span class="line">**Question**: Build your neural network model <span class="keyword">in</span> `nn_model()`.</span><br><span class="line"></span><br><span class="line">**Instructions**: The neural network model has to use the previous functions <span class="keyword">in</span> the right order.</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: nn_model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- dataset of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    num_iterations -- Number of iterations in gradient descent loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 5 lines of code)</span></span><br><span class="line">    parameters = initialize_parameters(n_x,n_h,n_y)</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">         </span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">        <span class="comment"># Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".</span></span><br><span class="line">        A2, cache = </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".</span></span><br><span class="line">        cost = compute_cost(A2,Y,parameters)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".</span></span><br><span class="line">        grads = backward_propagation(parameters,cache,X,Y)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".</span></span><br><span class="line">        parameters = update_parameters(parameters,grads)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess = nn_model_test_case()</span><br><span class="line">parameters = nn_model(X_assess, Y_assess, <span class="number">4</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>Cost after iteration 0: 0.692739
Cost after iteration 1000: 0.000218
Cost after iteration 2000: 0.000107
Cost after iteration 3000: 0.000071
Cost after iteration 4000: 0.000053
Cost after iteration 5000: 0.000042
Cost after iteration 6000: 0.000035
Cost after iteration 7000: 0.000030
Cost after iteration 8000: 0.000026
Cost after iteration 9000: 0.000023
W1 = [[-0.65848169  1.21866811]
 [-0.76204273  1.39377573]
 [ 0.5792005  -1.10397703]
 [ 0.76773391 -1.41477129]]
b1 = [[ 0.287592  ]
 [ 0.3511264 ]
 [-0.2431246 ]
 [-0.35772805]]
W2 = [[-2.45566237 -3.27042274  2.00784958  3.36773273]]
b2 = [[ 0.20459656]]</code></pre><p><strong>Expected Output</strong>:</p>
<table style="width:90%">

<tr> 
    <td> 
        **cost after iteration 0**
    </td>
    <td> 
        0.692739
    </td>
</tr>

<tr> 
    <td> 
        <center> $\vdots$ </center>
    </td>
    <td> 
        <center> $\vdots$ </center>
    </td>
</tr>

  <tr>
    <td>**W1**</td>
    <td> [[-0.65848169  1.21866811]
 [-0.76204273  1.39377573]
 [ 0.5792005  -1.10397703]
 [ 0.76773391 -1.41477129]]</td> 
  </tr>

  <tr>
    <td>**b1**</td>
    <td> [[ 0.287592  ]
 [ 0.3511264 ]
 [-0.2431246 ]
 [-0.35772805]] </td> 
  </tr>

  <tr>
    <td>**W2**</td>
    <td> [[-2.45566237 -3.27042274  2.00784958  3.36773273]] </td> 
  </tr>


  <tr>
    <td>**b2**</td>
    <td> [[ 0.20459656]] </td> 
  </tr>

</table>  

<h3 id="4-5-Predictions"><a href="#4-5-Predictions" class="headerlink" title="4.5 Predictions"></a>4.5 Predictions</h3><p><strong>Question</strong>: Use your model to predict by building predict().<br>Use forward propagation to predict results.</p>
<p><strong>Reminder</strong>: predictions = $y_{prediction} = \mathbb 1 \textfalse = \begin{cases}<br>      1 &amp; \text{if}\ activation &gt; 0.5 \<br>      0 &amp; \text{otherwise}<br>    \end{cases}$  </p>
<p>As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: <code>X_new = (X &gt; threshold)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: predict</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Using the learned parameters, predicts a class for each example in X</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    predictions -- vector of predictions of our model (red: 0 / blue: 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A2, cache = forward_propagation(X,parameters)</span><br><span class="line">    predictions = A2&gt;<span class="number">0.5</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">parameters, X_assess = predict_test_case()</span><br><span class="line"></span><br><span class="line">predictions = predict(parameters, X_assess)</span><br><span class="line">print(<span class="string">"predictions mean = "</span> + str(np.mean(predictions)))</span><br></pre></td></tr></table></figure>

<pre><code>predictions mean = 0.666666666667</code></pre><p><strong>Expected Output</strong>: </p>
<table style="width:40%">
  <tr>
    <td>**predictions mean**</td>
    <td> 0.666666666667 </td> 
  </tr>

</table>

<p>It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build a model with a n_h-dimensional hidden layer</span></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the decision boundary</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>

<pre><code>Cost after iteration 0: 0.693048
Cost after iteration 1000: 0.288083
Cost after iteration 2000: 0.254385
Cost after iteration 3000: 0.233864
Cost after iteration 4000: 0.226792
Cost after iteration 5000: 0.222644
Cost after iteration 6000: 0.219731
Cost after iteration 7000: 0.217504
Cost after iteration 8000: 0.219454
Cost after iteration 9000: 0.218607





&lt;matplotlib.text.Text at 0x7fce14c1b898&gt;</code></pre><p><img src="output_50_2.png" alt="png"></p>
<p><strong>Expected Output</strong>:</p>
<table style="width:40%">
  <tr>
    <td>**Cost after iteration 9000**</td>
    <td> 0.218607 </td> 
  </tr>

</table>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy: %d'</span> % float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy: 90%</code></pre><p><strong>Expected Output</strong>: </p>
<table style="width:15%">
  <tr>
    <td>**Accuracy**</td>
    <td> 90% </td> 
  </tr>
</table>

<p>Accuracy is really high compared to Logistic Regression. The model has learnt the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression. </p>
<p>Now, let’s try out several hidden layer sizes.</p>
<h3 id="4-6-Tuning-hidden-layer-size-optional-ungraded-exercise"><a href="#4-6-Tuning-hidden-layer-size-optional-ungraded-exercise" class="headerlink" title="4.6 - Tuning hidden layer size (optional/ungraded exercise)"></a>4.6 - Tuning hidden layer size (optional/ungraded exercise)</h3><p>Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This may take about 2 minutes to run</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>]</span><br><span class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> enumerate(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">'Hidden Layer of size %d'</span> % n_h)</span><br><span class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>)</span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    accuracy = float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy for &#123;&#125; hidden units: &#123;&#125; %"</span>.format(n_h, accuracy))</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy for 1 hidden units: 67.5 %
Accuracy for 2 hidden units: 67.25 %
Accuracy for 3 hidden units: 90.75 %
Accuracy for 4 hidden units: 90.5 %
Accuracy for 5 hidden units: 91.25 %
Accuracy for 20 hidden units: 90.0 %
Accuracy for 50 hidden units: 90.25 %</code></pre><p><img src="output_56_1.png" alt="png"></p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. </li>
<li>The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to  fits the data well without also incurring noticable overfitting.</li>
<li>You will also learn later about regularization, which lets you use very large models (such as n_h = 50) without much overfitting. </li>
</ul>
<p><strong>Optional questions</strong>:</p>
<p><strong>Note</strong>: Remember to submit the assignment but clicking the blue “Submit Assignment” button at the upper-right. </p>
<p>Some optional/ungraded questions that you can explore if you wish: </p>
<ul>
<li>What happens when you change the tanh activation for a sigmoid activation or a ReLU activation?</li>
<li>Play with the learning_rate. What happens?</li>
<li>What if we change the dataset? (See part 5 below!)</li>
</ul>
<font color='blue'>
**You've learnt to:**
- Build a complete neural network with a hidden layer
- Make a good use of a non-linear unit
- Implemented forward propagation and backpropagation, and trained a neural network
- See the impact of varying the hidden layer size, including overfitting.

<p>Nice work! </p>
<h2 id="5-Performance-on-other-datasets"><a href="#5-Performance-on-other-datasets" class="headerlink" title="5) Performance on other datasets"></a>5) Performance on other datasets</h2><p>If you want, you can rerun the whole notebook (minus the dataset part) for each of the following datasets.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Datasets</span></span><br><span class="line">noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()</span><br><span class="line"></span><br><span class="line">datasets = &#123;<span class="string">"noisy_circles"</span>: noisy_circles,</span><br><span class="line">            <span class="string">"noisy_moons"</span>: noisy_moons,</span><br><span class="line">            <span class="string">"blobs"</span>: blobs,</span><br><span class="line">            <span class="string">"gaussian_quantiles"</span>: gaussian_quantiles&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE ### (choose your dataset)</span></span><br><span class="line"><span class="comment"># dataset = "noisy_moons"</span></span><br><span class="line">dataset=<span class="string">"blobs"</span></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">X, Y = datasets[dataset]</span><br><span class="line">X, Y = X.T, Y.reshape(<span class="number">1</span>, Y.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># make blobs binary</span></span><br><span class="line"><span class="keyword">if</span> dataset == <span class="string">"blobs"</span>:</span><br><span class="line">    Y = Y%<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the data</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y, s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br></pre></td></tr></table></figure>


<p><img src="output_63_0.png" alt="png"></p>
<p>Congrats on finishing this Programming Assignment!</p>
<p>Reference:</p>
<ul>
<li><a href="http://scs.ryerson.ca/~aharley/neural-networks/" target="_blank" rel="noopener">http://scs.ryerson.ca/~aharley/neural-networks/</a></li>
<li><a href="http://cs231n.github.io/neural-networks-case-study/" target="_blank" rel="noopener">http://cs231n.github.io/neural-networks-case-study/</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/10/coursera%E7%BB%83%E4%B9%A0%E2%80%94%E2%80%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BB%BA%E7%AB%8B1/" data-id="ck7t8trek0008k6s68guxbe44" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/" rel="tag">deep learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-coursera练习——神经网络的建立" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/10/coursera%E7%BB%83%E4%B9%A0%E2%80%94%E2%80%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BB%BA%E7%AB%8B/" class="article-date">
  <time datetime="2018-12-10T10:55:34.000Z" itemprop="datePublished">2018-12-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/10/coursera%E7%BB%83%E4%B9%A0%E2%80%94%E2%80%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BB%BA%E7%AB%8B/">coursera练习——神经网络的建立</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Planar-data-classification-with-one-hidden-layer"><a href="#Planar-data-classification-with-one-hidden-layer" class="headerlink" title="Planar data classification with one hidden layer"></a>Planar data classification with one hidden layer</h1><p>Welcome to your week 3 programming assignment. It’s time to build your first neural network, which will have a hidden layer. You will see a big difference between this model and the one you implemented using logistic regression. </p>
<p><strong>You will learn how to:</strong></p>
<ul>
<li>Implement a 2-class classification neural network with a single hidden layer</li>
<li>Use units with a non-linear activation function, such as tanh </li>
<li>Compute the cross entropy loss </li>
<li>Implement forward and backward propagation</li>
</ul>
<h2 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1 - Packages"></a>1 - Packages</h2><p>Let’s first import all the packages that you will need during this assignment.</p>
<ul>
<li><a href="www.numpy.org">numpy</a> is the fundamental package for scientific computing with Python.</li>
<li><a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">sklearn</a> provides simple and efficient tools for data mining and data analysis. </li>
<li><a href="http://matplotlib.org" target="_blank" rel="noopener">matplotlib</a> is a library for plotting graphs in Python.</li>
<li>testCases provides some test examples to assess the correctness of your functions</li>
<li>planar_utils provide various useful functions used in this assignment</li>
</ul>
<h1 id="Package-imports"><a href="#Package-imports" class="headerlink" title="Package imports"></a>Package imports</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from testCases_v2 import *</span><br><span class="line">import sklearn</span><br><span class="line">import sklearn.datasets</span><br><span class="line">import sklearn.linear_model</span><br><span class="line">from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">np.random.seed(1) # set a seed so that the results are consistent</span><br></pre></td></tr></table></figure>
<h2 id="2-Dataset"><a href="#2-Dataset" class="headerlink" title="2 - Dataset"></a>2 - Dataset</h2><p>First, let’s get the dataset you will work on. The following code will load a “flower” 2-class dataset into variables <code>X</code> and <code>Y</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X, Y &#x3D; load_planar_dataset()</span><br></pre></td></tr></table></figure>

<p>Visualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Visualize the data:</span><br><span class="line">plt.scatter(X[0, :], X[1, :], c&#x3D;Y, s&#x3D;40, cmap&#x3D;plt.cm.Spectral);</span><br></pre></td></tr></table></figure>

<p><img src="1.png" alt="flower"></p>
<p>You have:<br>    - a numpy-array (matrix) X that contains your features (x1, x2)<br>    - a numpy-array (vector) Y that contains your labels (red:0, blue:1).</p>
<p>Lets first get a better sense of what our data is like. </p>
<p><strong>Exercise</strong>: How many training examples do you have? In addition, what is the <code>shape</code> of the variables <code>X</code> and <code>Y</code>? </p>
<p><strong>Hint</strong>: How do you get the shape of a numpy array? <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html" target="_blank" rel="noopener">(help)</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">### START CODE HERE ### (≈ 3 lines of code)</span><br><span class="line">shape_X &#x3D; X.shape</span><br><span class="line">shape_Y &#x3D; Y.shape</span><br><span class="line">m &#x3D; shape_X[1] # training set size</span><br><span class="line">### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">print (&#39;The shape of X is: &#39; + str(shape_X))</span><br><span class="line">print (&#39;The shape of Y is: &#39; + str(shape_Y))</span><br><span class="line">print (&#39;I have m &#x3D; %d training examples!&#39; % (m))</span><br></pre></td></tr></table></figure>

<p><strong>Expected Output</strong>:</p>
<table style="width:20%">

  <tr>
    <td>**shape of X**</td>
    <td> (2, 400) </td> 
  </tr>

  <tr>
    <td>**shape of Y**</td>
    <td>(1, 400) </td> 
  </tr>

<pre><code>&lt;tr&gt;
&lt;td&gt;**m**&lt;/td&gt;
&lt;td&gt; 400 &lt;/td&gt; </code></pre>  </tr>

</table>

<h2 id="3-Simple-Logistic-Regression"><a href="#3-Simple-Logistic-Regression" class="headerlink" title="3 - Simple Logistic Regression"></a>3 - Simple Logistic Regression</h2><p>Before building a full neural network, lets first see how logistic regression performs on this problem. You can use sklearn’s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Train the logistic regression classifier</span><br><span class="line">clf &#x3D; sklearn.linear_model.LogisticRegressionCV();</span><br><span class="line">clf.fit(X.T, Y.T);</span><br></pre></td></tr></table></figure>

<p>You can now plot the decision boundary of these models. Run the code below.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Plot the decision boundary for logistic regression</span><br><span class="line">plot_decision_boundary(lambda x: clf.predict(x), X, Y)</span><br><span class="line">plt.title(&quot;Logistic Regression&quot;)</span><br><span class="line"></span><br><span class="line"># Print accuracy</span><br><span class="line">LR_predictions &#x3D; clf.predict(X.T)</span><br><span class="line">print (&#39;Accuracy of logistic regression: %d &#39; % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))&#x2F;float(Y.size)*100) +</span><br><span class="line">       &#39;% &#39; + &quot;(percentage of correctly labelled datapoints)&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>Expected Output</strong>:</p>
<table style="width:20%">
  <tr>
    <td>**Accuracy**</td>
    <td> 47% </td> 
  </tr>

</table>

<p><img src="2.png" alt=""></p>
<p><strong>Interpretation</strong>: The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network will do better. Let’s try this now! </p>
<h2 id="4-Neural-Network-model"><a href="#4-Neural-Network-model" class="headerlink" title="4 - Neural Network model"></a>4 - Neural Network model</h2><p>Logistic regression did not work well on the “flower dataset”. You are going to train a Neural Network with a single hidden layer.</p>
<p><strong>Here is our model</strong>:<br><img src="images/classification_kiank.png" style="width:600px;height:300px;"></p>
<p><strong>Mathematically</strong>:</p>
<p>For one example $x^{(i)}$:<br>$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\tag{1}$$<br>$$a^{[1] (i)} = \tanh(z^{[1] (i)})\tag{2}$$<br>$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\tag{3}$$<br>$$\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})\tag{4}$$<br>$$y^{(i)}_{prediction} = \begin{cases} 1 &amp; \mbox{if } a^{<a href="i">2</a>} &gt; 0.5 \ 0 &amp; \mbox{otherwise } \end{cases}\tag{5}$$</p>
<p>Given the predictions on all the examples, you can also compute the cost $J$ as follows:<br>$$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right)  \large  \right) \small \tag{6}$$</p>
<p><strong>Reminder</strong>: The general methodology to build a Neural Network is to:<br>    1. Define the neural network structure ( # of input units,  # of hidden units, etc).<br>    2. Initialize the model’s parameters<br>    3. Loop:<br>        - Implement forward propagation<br>        - Compute loss<br>        - Implement backward propagation to get the gradients<br>        - Update parameters (gradient descent)</p>
<p>You often build helper functions to compute steps 1-3 and then merge them into one function we call <code>nn_model()</code>. Once you’ve built <code>nn_model()</code> and learnt the right parameters, you can make predictions on new data.</p>
<h3 id="4-1-Defining-the-neural-network-structure"><a href="#4-1-Defining-the-neural-network-structure" class="headerlink" title="4.1 - Defining the neural network structure"></a>4.1 - Defining the neural network structure</h3><p><strong>Exercise</strong>: Define three variables:<br>    - n_x: the size of the input layer<br>    - n_h: the size of the hidden layer (set this to 4)<br>    - n_y: the size of the output layer</p>
<p><strong>Hint</strong>: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: layer_sizes</span><br><span class="line"></span><br><span class="line">def layer_sizes(X, Y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input dataset of shape (input size, number of examples)</span><br><span class="line">    Y -- labels of shape (output size, number of examples)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    n_x -- the size of the input layer</span><br><span class="line">    n_h -- the size of the hidden layer</span><br><span class="line">    n_y -- the size of the output layer</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    ### START CODE HERE ### (≈ 3 lines of code)</span><br><span class="line">    n_x &#x3D; X.shape[0] # size of input layer</span><br><span class="line">    n_h &#x3D; 4</span><br><span class="line">    n_y &#x3D; Y.shape[0] # size of output layer</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    return (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess &#x3D; layer_sizes_test_case()</span><br><span class="line">(n_x, n_h, n_y) &#x3D; layer_sizes(X_assess, Y_assess)</span><br><span class="line">print(&quot;The size of the input layer is: n_x &#x3D; &quot; + str(n_x))</span><br><span class="line">print(&quot;The size of the hidden layer is: n_h &#x3D; &quot; + str(n_h))</span><br><span class="line">print(&quot;The size of the output layer is: n_y &#x3D; &quot; + str(n_y))</span><br></pre></td></tr></table></figure>


<p><strong>Expected Output</strong> (these are not the sizes you will use for your network, they are just used to assess the function you’ve just coded).</p>
<table style="width:20%">
  <tr>
    <td>**n_x**</td>
    <td> 5 </td> 
  </tr>

<pre><code>&lt;tr&gt;
&lt;td&gt;**n_h**&lt;/td&gt;
&lt;td&gt; 4 &lt;/td&gt; </code></pre>  </tr>

<pre><code>&lt;tr&gt;
&lt;td&gt;**n_y**&lt;/td&gt;
&lt;td&gt; 2 &lt;/td&gt; </code></pre>  </tr>

</table>

<h3 id="4-2-Initialize-the-model’s-parameters"><a href="#4-2-Initialize-the-model’s-parameters" class="headerlink" title="4.2 - Initialize the model’s parameters"></a>4.2 - Initialize the model’s parameters</h3><p><strong>Exercise</strong>: Implement the function <code>initialize_parameters()</code>.</p>
<p><strong>Instructions</strong>:</p>
<ul>
<li>Make sure your parameters’ sizes are right. Refer to the neural network figure above if needed.</li>
<li>You will initialize the weights matrices with random values. <ul>
<li>Use: <code>np.random.randn(a,b) * 0.01</code> to randomly initialize a matrix of shape (a,b).</li>
</ul>
</li>
<li>You will initialize the bias vectors as zeros. <ul>
<li>Use: <code>np.zeros((a,b))</code> to initialize a matrix of shape (a,b) with zeros.</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: initialize_parameters</span><br><span class="line"></span><br><span class="line">def initialize_parameters(n_x, n_h, n_y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Argument:</span><br><span class="line">    n_x -- size of the input layer</span><br><span class="line">    n_h -- size of the hidden layer</span><br><span class="line">    n_y -- size of the output layer</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    params -- python dictionary containing your parameters:</span><br><span class="line">                    W1 -- weight matrix of shape (n_h, n_x)</span><br><span class="line">                    b1 -- bias vector of shape (n_h, 1)</span><br><span class="line">                    W2 -- weight matrix of shape (n_y, n_h)</span><br><span class="line">                    b2 -- bias vector of shape (n_y, 1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.</span><br><span class="line">    </span><br><span class="line">    ### START CODE HERE ### (≈ 4 lines of code)</span><br><span class="line">    W1 &#x3D; np.random.randn(n_h,n_x)*0.01</span><br><span class="line">    b1 &#x3D; np.zeros((n_h,1))</span><br><span class="line">    W2 &#x3D; np.random.randn(n_y,n_h)*0.01</span><br><span class="line">    b2 &#x3D; np.zeros((n_y,1))</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    assert (W1.shape &#x3D;&#x3D; (n_h, n_x))</span><br><span class="line">    assert (b1.shape &#x3D;&#x3D; (n_h, 1))</span><br><span class="line">    assert (W2.shape &#x3D;&#x3D; (n_y, n_h))</span><br><span class="line">    assert (b2.shape &#x3D;&#x3D; (n_y, 1))</span><br><span class="line">    </span><br><span class="line">    parameters &#x3D; &#123;&quot;W1&quot;: W1,</span><br><span class="line">                  &quot;b1&quot;: b1,</span><br><span class="line">                  &quot;W2&quot;: W2,</span><br><span class="line">                  &quot;b2&quot;: b2&#125;</span><br><span class="line">    </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n_x, n_h, n_y &#x3D; initialize_parameters_test_case()</span><br><span class="line"></span><br><span class="line">parameters &#x3D; initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">print(&quot;W1 &#x3D; &quot; + str(parameters[&quot;W1&quot;]))</span><br><span class="line">print(&quot;b1 &#x3D; &quot; + str(parameters[&quot;b1&quot;]))</span><br><span class="line">print(&quot;W2 &#x3D; &quot; + str(parameters[&quot;W2&quot;]))</span><br><span class="line">print(&quot;b2 &#x3D; &quot; + str(parameters[&quot;b2&quot;]))</span><br></pre></td></tr></table></figure>

<p><strong>Expected Output</strong>:</p>
<table style="width:90%">
  <tr>
    <td>**W1**</td>
    <td> [[-0.00416758 -0.00056267]
 [-0.02136196  0.01640271]
 [-0.01793436 -0.00841747]
 [ 0.00502881 -0.01245288]] </td> 
  </tr>

  <tr>
    <td>**b1**</td>
    <td> [[ 0.]
 [ 0.]
 [ 0.]
 [ 0.]] </td> 
  </tr>

  <tr>
    <td>**W2**</td>
    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td> 
  </tr>


  <tr>
    <td>**b2**</td>
    <td> [[ 0.]] </td> 
  </tr>

</table>

<h3 id="4-3-The-Loop"><a href="#4-3-The-Loop" class="headerlink" title="4.3 - The Loop"></a>4.3 - The Loop</h3><p><strong>Question</strong>: Implement <code>forward_propagation()</code>.</p>
<p><strong>Instructions</strong>:</p>
<ul>
<li>Look above at the mathematical representation of your classifier.</li>
<li>You can use the function <code>sigmoid()</code>. It is built-in (imported) in the notebook.</li>
<li>You can use the function <code>np.tanh()</code>. It is part of the numpy library.</li>
<li>The steps you have to implement are:<ol>
<li>Retrieve each parameter from the dictionary “parameters” (which is the output of <code>initialize_parameters()</code>) by using <code>parameters[&quot;..&quot;]</code>.</li>
<li>Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).</li>
</ol>
</li>
<li>Values needed in the backpropagation are stored in “<code>cache</code>“. The <code>cache</code> will be given as an input to the backpropagation function.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: forward_propagation</span><br><span class="line"></span><br><span class="line">def forward_propagation(X, parameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Argument:</span><br><span class="line">    X -- input data of size (n_x, m)</span><br><span class="line">    parameters -- python dictionary containing your parameters (output of initialization function)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    A2 -- The sigmoid output of the second activation</span><br><span class="line">    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Retrieve each parameter from the dictionary &quot;parameters&quot;</span><br><span class="line">    ### START CODE HERE ### (≈ 4 lines of code)</span><br><span class="line">    W1 &#x3D; parameters[&quot;W1&quot;]</span><br><span class="line">    b1 &#x3D; parameters[&quot;b1&quot;]</span><br><span class="line">    W2 &#x3D; parameters[&quot;W2&quot;]</span><br><span class="line">    b2 &#x3D; parameters[&quot;b2&quot;]</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Implement Forward Propagation to calculate A2 (probabilities)</span><br><span class="line">    ### START CODE HERE ### (≈ 4 lines of code)</span><br><span class="line">    Z1 &#x3D; np.dot(W1,X)+b1</span><br><span class="line">    A1 &#x3D; np.tanh(Z1)</span><br><span class="line">    Z2 &#x3D; np.dot(W2,A1)+b2</span><br><span class="line">    A2 &#x3D; sigmoid(Z2)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    assert(A2.shape &#x3D;&#x3D; (1, X.shape[1]))</span><br><span class="line">    </span><br><span class="line">    cache &#x3D; &#123;&quot;Z1&quot;: Z1,</span><br><span class="line">             &quot;A1&quot;: A1,</span><br><span class="line">             &quot;Z2&quot;: Z2,</span><br><span class="line">             &quot;A2&quot;: A2&#125;</span><br><span class="line">    </span><br><span class="line">    return A2, cache</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_assess, parameters &#x3D; forward_propagation_test_case()</span><br><span class="line">A2, cache &#x3D; forward_propagation(X_assess, parameters)</span><br><span class="line"></span><br><span class="line"># Note: we use the mean here just to make sure that your output matches ours. </span><br><span class="line">print(np.mean(cache[&#39;Z1&#39;]) ,np.mean(cache[&#39;A1&#39;]),np.mean(cache[&#39;Z2&#39;]),np.mean(cache[&#39;A2&#39;]))</span><br></pre></td></tr></table></figure>
<p><strong>Expected Output</strong>:</p>
<table style="width:50%">
  <tr>
    <td> 0.262818640198 0.091999045227 -1.30766601287 0.212877681719 </td> 
  </tr>
</table>

<p>Now that you have computed $A^{[2]}$ (in the Python variable “<code>A2</code>“), which contains $a^{<a href="i">2</a>}$ for every example, you can compute the cost function as follows:</p>
<p>$$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small\tag{13}$$</p>
<p><strong>Exercise</strong>: Implement <code>compute_cost()</code> to compute the value of the cost $J$.</p>
<p><strong>Instructions</strong>:</p>
<ul>
<li>There are many ways to implement the cross-entropy loss. To help you, we give you how we would have implemented<br>$- \sum\limits_{i=0}^{m}  y^{(i)}\log(a^{<a href="i">2</a>})$:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">logprobs = np.multiply(np.log(A2),Y)</span><br><span class="line">cost = - np.sum(logprobs)                <span class="comment"># no need to use a for loop!</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>(you can use either <code>np.multiply()</code> and then <code>np.sum()</code> or directly <code>np.dot()</code>).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: compute_cost</span><br><span class="line"></span><br><span class="line">def compute_cost(A2, Y, parameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Computes the cross-entropy cost given in equation (13)</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)</span><br><span class="line">    Y -- &quot;true&quot; labels vector of shape (1, number of examples)</span><br><span class="line">    parameters -- python dictionary containing your parameters W1, b1, W2 and b2</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    cost -- cross-entropy cost given equation (13)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    m &#x3D; Y.shape[1] # number of example</span><br><span class="line"></span><br><span class="line">    # Compute the cross-entropy cost</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    logprobs &#x3D; np.multiply(np.log(A2),Y)+np.multiply(np.log(1-A2),(1-Y))</span><br><span class="line">    cost &#x3D; -1&#x2F;m*np.sum(logprobs)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    cost &#x3D; np.squeeze(cost)     # makes sure cost is the dimension we expect. </span><br><span class="line">                                # E.g., turns [[17]] into 17 </span><br><span class="line">    assert(isinstance(cost, float))</span><br><span class="line">    </span><br><span class="line">    return cost</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A2, Y_assess, parameters &#x3D; compute_cost_test_case()</span><br><span class="line"></span><br><span class="line">print(&quot;cost &#x3D; &quot; + str(compute_cost(A2, Y_assess, parameters)))</span><br></pre></td></tr></table></figure>

<p><strong>Expected Output</strong>:</p>
<table style="width:20%">
  <tr>
    <td>**cost**</td>
    <td> 0.693058761... </td> 
  </tr>

</table>

<p>Using the cache computed during forward propagation, you can now implement backward propagation.</p>
<p><strong>Question</strong>: Implement the function <code>backward_propagation()</code>.</p>
<p><strong>Instructions</strong>:<br>Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.  </p>
<img src="images/grad_summary.png" style="width:600px;height:300px;">

<!--
$\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } = \frac{1}{m} (a^{[2](i)} - y^{(i)})$

$\frac{\partial \mathcal{J} }{ \partial W_2 } = \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } a^{[1] (i) T} $

$\frac{\partial \mathcal{J} }{ \partial b_2 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)}}}$

$\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} } =  W_2^T \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $

$\frac{\partial \mathcal{J} }{ \partial W_1 } = \frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} }  X^T $

$\frac{\partial \mathcal{J} _i }{ \partial b_1 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)}}}$

- Note that $*$ denotes elementwise multiplication.
- The notation you will use is common in deep learning coding:
    - dW1 = $\frac{\partial \mathcal{J} }{ \partial W_1 }$
    - db1 = $\frac{\partial \mathcal{J} }{ \partial b_1 }$
    - dW2 = $\frac{\partial \mathcal{J} }{ \partial W_2 }$
    - db2 = $\frac{\partial \mathcal{J} }{ \partial b_2 }$

!-->

<ul>
<li>Tips:<ul>
<li>To compute dZ1 you’ll need to compute $g^{[1]’}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]’}(z) = 1-a^2$. So you can compute<br>$g^{[1]’}(Z^{[1]})$ using <code>(1 - np.power(A1, 2))</code>.</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: backward_propagation</span><br><span class="line"></span><br><span class="line">def backward_propagation(parameters, cache, X, Y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Implement the backward propagation using the instructions above.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing our parameters </span><br><span class="line">    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;.</span><br><span class="line">    X -- input data of shape (2, number of examples)</span><br><span class="line">    Y -- &quot;true&quot; labels vector of shape (1, number of examples)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    grads -- python dictionary containing your gradients with respect to different parameters</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    m &#x3D; X.shape[1]</span><br><span class="line">    </span><br><span class="line">    # First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    W1 &#x3D; parameters[&quot;W1&quot;]</span><br><span class="line">    W2 &#x3D; parameters[&quot;W2&quot;]</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">    # Retrieve also A1 and A2 from dictionary &quot;cache&quot;.</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    A1 &#x3D; cache[&quot;A1&quot;]</span><br><span class="line">    A2 &#x3D; cache[&quot;A2&quot;]</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Backward propagation: calculate dW1, db1, dW2, db2. </span><br><span class="line">    ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)</span><br><span class="line">    dZ2 &#x3D; A2-Y</span><br><span class="line">    dW2 &#x3D; 1&#x2F;m*np.dot(dZ2,A1.T)</span><br><span class="line">    db2 &#x3D; 1&#x2F;m*np.sum(dZ2,axis&#x3D;1,keepdims&#x3D;True)</span><br><span class="line">    dZ1 &#x3D; np.dot(W2.T,dZ2)*(1-np.power(A1,2))</span><br><span class="line">    dW1 &#x3D; 1&#x2F;m*np.dot(dZ1,X.T)</span><br><span class="line">    db1 &#x3D; 1&#x2F;m*np.sum(dZ1,axis&#x3D;1,keepdims&#x3D;True)</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    grads &#x3D; &#123;&quot;dW1&quot;: dW1,</span><br><span class="line">             &quot;db1&quot;: db1,</span><br><span class="line">             &quot;dW2&quot;: dW2,</span><br><span class="line">             &quot;db2&quot;: db2&#125;</span><br><span class="line">    </span><br><span class="line">    return grads</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, cache, X_assess, Y_assess &#x3D; backward_propagation_test_case()</span><br><span class="line"></span><br><span class="line">grads &#x3D; backward_propagation(parameters, cache, X_assess, Y_assess)</span><br><span class="line">print (&quot;dW1 &#x3D; &quot;+ str(grads[&quot;dW1&quot;]))</span><br><span class="line">print (&quot;db1 &#x3D; &quot;+ str(grads[&quot;db1&quot;]))</span><br><span class="line">print (&quot;dW2 &#x3D; &quot;+ str(grads[&quot;dW2&quot;]))</span><br><span class="line">print (&quot;db2 &#x3D; &quot;+ str(grads[&quot;db2&quot;]))</span><br></pre></td></tr></table></figure>

<p><strong>Expected output</strong>:</p>
<table style="width:80%">
  <tr>
    <td>**dW1**</td>
    <td> [[ 0.00301023 -0.00747267]
 [ 0.00257968 -0.00641288]
 [-0.00156892  0.003893  ]
 [-0.00652037  0.01618243]] </td> 
  </tr>

  <tr>
    <td>**db1**</td>
    <td>  [[ 0.00176201]
 [ 0.00150995]
 [-0.00091736]
 [-0.00381422]] </td> 
  </tr>

  <tr>
    <td>**dW2**</td>
    <td> [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]] </td> 
  </tr>


  <tr>
    <td>**db2**</td>
    <td> [[-0.16655712]] </td> 
  </tr>

</table>  

<p><strong>Question</strong>: Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).</p>
<p><strong>General gradient descent rule</strong>: $ \theta = \theta - \alpha \frac{\partial J }{ \partial \theta }$ where $\alpha$ is the learning rate and $\theta$ represents a parameter.</p>
<p><strong>Illustration</strong>: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.</p>
<p><img src="images/sgd.gif" style="width:400;height:400;"> <img src="images/sgd_bad.gif" style="width:400;height:400;"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: update_parameters</span><br><span class="line"></span><br><span class="line">def update_parameters(parameters, grads, learning_rate &#x3D; 1.2):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Updates parameters using the gradient descent update rule given above</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters </span><br><span class="line">    grads -- python dictionary containing your gradients </span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- python dictionary containing your updated parameters </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # Retrieve each parameter from the dictionary &quot;parameters&quot;</span><br><span class="line">    ### START CODE HERE ### (≈ 4 lines of code)</span><br><span class="line">    W1 &#x3D; parameters[&quot;W1&quot;]</span><br><span class="line">    b1 &#x3D; parameters[&quot;b1&quot;]</span><br><span class="line">    W2 &#x3D; parameters[&quot;W2&quot;]</span><br><span class="line">    b2 &#x3D; parameters[&quot;b2&quot;]</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Retrieve each gradient from the dictionary &quot;grads&quot;</span><br><span class="line">    ### START CODE HERE ### (≈ 4 lines of code)</span><br><span class="line">    dW1 &#x3D; grads[&quot;dW1&quot;]</span><br><span class="line">    db1 &#x3D; grads[&quot;db1&quot;]</span><br><span class="line">    dW2 &#x3D; grads[&quot;dW2&quot;]</span><br><span class="line">    db2 &#x3D; grads[&quot;db2&quot;]</span><br><span class="line">    ## END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Update rule for each parameter</span><br><span class="line">    ### START CODE HERE ### (≈ 4 lines of code)</span><br><span class="line">    W1 &#x3D; W1-learning_rate*dW1</span><br><span class="line">    b1 &#x3D; b1-learning_rate*db1</span><br><span class="line">    W2 &#x3D; W2-learning_rate*dW2</span><br><span class="line">    b2 &#x3D; b2-learning_rate*db2</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    parameters &#x3D; &#123;&quot;W1&quot;: W1,</span><br><span class="line">                  &quot;b1&quot;: b1,</span><br><span class="line">                  &quot;W2&quot;: W2,</span><br><span class="line">                  &quot;b2&quot;: b2&#125;</span><br><span class="line">    </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads &#x3D; update_parameters_test_case()</span><br><span class="line">parameters &#x3D; update_parameters(parameters, grads)</span><br><span class="line"></span><br><span class="line">print(&quot;W1 &#x3D; &quot; + str(parameters[&quot;W1&quot;]))</span><br><span class="line">print(&quot;b1 &#x3D; &quot; + str(parameters[&quot;b1&quot;]))</span><br><span class="line">print(&quot;W2 &#x3D; &quot; + str(parameters[&quot;W2&quot;]))</span><br><span class="line">print(&quot;b2 &#x3D; &quot; + str(parameters[&quot;b2&quot;]))</span><br></pre></td></tr></table></figure>

<p><strong>Expected Output</strong>:</p>
<table style="width:80%">
  <tr>
    <td>**W1**</td>
    <td> [[-0.00643025  0.01936718]
 [-0.02410458  0.03978052]
 [-0.01653973 -0.02096177]
 [ 0.01046864 -0.05990141]]</td> 
  </tr>

  <tr>
    <td>**b1**</td>
    <td> [[ -1.02420756e-06]
 [  1.27373948e-05]
 [  8.32996807e-07]
 [ -3.20136836e-06]]</td> 
  </tr>

  <tr>
    <td>**W2**</td>
    <td> [[-0.01041081 -0.04463285  0.01758031  0.04747113]] </td> 
  </tr>


  <tr>
    <td>**b2**</td>
    <td> [[ 0.00010457]] </td> 
  </tr>

</table>  

<h3 id="4-4-Integrate-parts-4-1-4-2-and-4-3-in-nn-model"><a href="#4-4-Integrate-parts-4-1-4-2-and-4-3-in-nn-model" class="headerlink" title="4.4 - Integrate parts 4.1, 4.2 and 4.3 in nn_model()"></a>4.4 - Integrate parts 4.1, 4.2 and 4.3 in nn_model()</h3><p><strong>Question</strong>: Build your neural network model in <code>nn_model()</code>.</p>
<p><strong>Instructions</strong>: The neural network model has to use the previous functions in the right order.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: nn_model</span><br><span class="line"></span><br><span class="line">def nn_model(X, Y, n_h, num_iterations &#x3D; 10000, print_cost&#x3D;False):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    X -- dataset of shape (2, number of examples)</span><br><span class="line">    Y -- labels of shape (1, number of examples)</span><br><span class="line">    n_h -- size of the hidden layer</span><br><span class="line">    num_iterations -- Number of iterations in gradient descent loop</span><br><span class="line">    print_cost -- if True, print the cost every 1000 iterations</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- parameters learnt by the model. They can then be used to predict.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(3)</span><br><span class="line">    n_x &#x3D; layer_sizes(X, Y)[0]</span><br><span class="line">    n_y &#x3D; layer_sizes(X, Y)[2]</span><br><span class="line">    </span><br><span class="line">    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: &quot;n_x, n_h, n_y&quot;. Outputs &#x3D; &quot;W1, b1, W2, b2, parameters&quot;.</span><br><span class="line">    ### START CODE HERE ### (≈ 5 lines of code)</span><br><span class="line">    parameters &#x3D; initialize_parameters(n_x,n_h,n_y)</span><br><span class="line">    W1 &#x3D; parameters[&quot;W1&quot;]</span><br><span class="line">    b1 &#x3D; parameters[&quot;b1&quot;]</span><br><span class="line">    W2 &#x3D; parameters[&quot;W2&quot;]</span><br><span class="line">    b2 &#x3D; parameters[&quot;b2&quot;]</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    # Loop (gradient descent)</span><br><span class="line"></span><br><span class="line">    for i in range(0, num_iterations):</span><br><span class="line">         </span><br><span class="line">        ### START CODE HERE ### (≈ 4 lines of code)</span><br><span class="line">        # Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;.</span><br><span class="line">        A2, cache &#x3D; </span><br><span class="line">        </span><br><span class="line">        # Cost function. Inputs: &quot;A2, Y, parameters&quot;. Outputs: &quot;cost&quot;.</span><br><span class="line">        cost &#x3D; compute_cost(A2,Y,parameters)</span><br><span class="line"> </span><br><span class="line">        # Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;.</span><br><span class="line">        grads &#x3D; backward_propagation(parameters,cache,X,Y)</span><br><span class="line"> </span><br><span class="line">        # Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;.</span><br><span class="line">        parameters &#x3D; update_parameters(parameters,grads)</span><br><span class="line">        </span><br><span class="line">        ### END CODE HERE ###</span><br><span class="line">        </span><br><span class="line">        # Print the cost every 1000 iterations</span><br><span class="line">        if print_cost and i % 1000 &#x3D;&#x3D; 0:</span><br><span class="line">            print (&quot;Cost after iteration %i: %f&quot; %(i, cost))</span><br><span class="line"></span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess &#x3D; nn_model_test_case()</span><br><span class="line">parameters &#x3D; nn_model(X_assess, Y_assess, 4, num_iterations&#x3D;10000, print_cost&#x3D;True)</span><br><span class="line">print(&quot;W1 &#x3D; &quot; + str(parameters[&quot;W1&quot;]))</span><br><span class="line">print(&quot;b1 &#x3D; &quot; + str(parameters[&quot;b1&quot;]))</span><br><span class="line">print(&quot;W2 &#x3D; &quot; + str(parameters[&quot;W2&quot;]))</span><br><span class="line">print(&quot;b2 &#x3D; &quot; + str(parameters[&quot;b2&quot;]))</span><br></pre></td></tr></table></figure>
<p><strong>Expected Output</strong>:<br><img src="3.png" alt=""></p>
<h3 id="4-5-Predictions"><a href="#4-5-Predictions" class="headerlink" title="4.5 Predictions"></a>4.5 Predictions</h3><p><strong>Question</strong>: Use your model to predict by building predict().<br>Use forward propagation to predict results.</p>
<p><strong>Reminder</strong>: predictions = $y_{prediction} = \mathbb 1 \textfalse = \begin{cases}<br>      1 &amp; \text{if}\ activation &gt; 0.5 \<br>      0 &amp; \text{otherwise}<br>    \end{cases}$  </p>
<p>As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: <code>X_new = (X &gt; threshold)</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># GRADED FUNCTION: predict</span><br><span class="line"></span><br><span class="line">def predict(parameters, X):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Using the learned parameters, predicts a class for each example in X</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    parameters -- python dictionary containing your parameters </span><br><span class="line">    X -- input data of size (n_x, m)</span><br><span class="line">    </span><br><span class="line">    Returns</span><br><span class="line">    predictions -- vector of predictions of our model (red: 0 &#x2F; blue: 1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Computes probabilities using forward propagation, and classifies to 0&#x2F;1 using 0.5 as the threshold.</span><br><span class="line">    ### START CODE HERE ### (≈ 2 lines of code)</span><br><span class="line">    A2, cache &#x3D; forward_propagation(X,parameters)</span><br><span class="line">    predictions &#x3D; A2&gt;0.5</span><br><span class="line">    ### END CODE HERE ###</span><br><span class="line">    </span><br><span class="line">    return predictions</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">parameters, X_assess &#x3D; predict_test_case()</span><br><span class="line"></span><br><span class="line">predictions &#x3D; predict(parameters, X_assess)</span><br><span class="line">print(&quot;predictions mean &#x3D; &quot; + str(np.mean(predictions)))</span><br></pre></td></tr></table></figure>

<p><strong>Expected Output</strong>: </p>
<table style="width:40%">
  <tr>
    <td>**predictions mean**</td>
    <td> 0.666666666667 </td> 
  </tr>

</table>

<p>It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Build a model with a n_h-dimensional hidden layer</span><br><span class="line">parameters &#x3D; nn_model(X, Y, n_h &#x3D; 4, num_iterations &#x3D; 10000, print_cost&#x3D;True)</span><br><span class="line"></span><br><span class="line"># Plot the decision boundary</span><br><span class="line">plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(&quot;Decision Boundary for hidden layer size &quot; + str(4))</span><br></pre></td></tr></table></figure>
<p><strong>Expected Output</strong>:</p>
<p>Cost after iteration 0: 0.693048<br>Cost after iteration 1000: 0.288083<br>Cost after iteration 2000: 0.254385<br>Cost after iteration 3000: 0.233864<br>Cost after iteration 4000: 0.226792<br>Cost after iteration 5000: 0.222644<br>Cost after iteration 6000: 0.219731<br>Cost after iteration 7000: 0.217504<br>Cost after iteration 8000: 0.219454<br>Cost after iteration 9000: 0.218607<br><img src="4.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Print accuracy</span><br><span class="line">predictions &#x3D; predict(parameters, X)</span><br><span class="line">print (&#39;Accuracy: %d&#39; % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))&#x2F;float(Y.size)*100) + &#39;%&#39;)</span><br></pre></td></tr></table></figure>
<p><strong>Expected Output</strong>: </p>
<table style="width:15%">
  <tr>
    <td>**Accuracy**</td>
    <td> 90% </td> 
  </tr>
</table>

<p>Accuracy is really high compared to Logistic Regression. The model has learnt the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression.</p>
<p>Now, let’s try out several hidden layer sizes.</p>
<h3 id="4-6-Tuning-hidden-layer-size-optional-ungraded-exercise"><a href="#4-6-Tuning-hidden-layer-size-optional-ungraded-exercise" class="headerlink" title="4.6 - Tuning hidden layer size (optional/ungraded exercise)"></a>4.6 - Tuning hidden layer size (optional/ungraded exercise)</h3><p>Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># This may take about 2 minutes to run</span><br><span class="line"></span><br><span class="line">plt.figure(figsize&#x3D;(16, 32))</span><br><span class="line">hidden_layer_sizes &#x3D; [1, 2, 3, 4, 5, 20, 50]</span><br><span class="line">for i, n_h in enumerate(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(5, 2, i+1)</span><br><span class="line">    plt.title(&#39;Hidden Layer of size %d&#39; % n_h)</span><br><span class="line">    parameters &#x3D; nn_model(X, Y, n_h, num_iterations &#x3D; 5000)</span><br><span class="line">    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)</span><br><span class="line">    predictions &#x3D; predict(parameters, X)</span><br><span class="line">    accuracy &#x3D; float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))&#x2F;float(Y.size)*100)</span><br><span class="line">    print (&quot;Accuracy for &#123;&#125; hidden units: &#123;&#125; %&quot;.format(n_h, accuracy))</span><br></pre></td></tr></table></figure>
<p><strong>Expected Output</strong>:<br>Accuracy for 1 hidden units: 67.5 %<br>Accuracy for 2 hidden units: 67.25 %<br>Accuracy for 3 hidden units: 90.75 %<br>Accuracy for 4 hidden units: 90.5 %<br>Accuracy for 5 hidden units: 91.25 %<br>Accuracy for 20 hidden units: 90.0 %<br>Accuracy for 50 hidden units: 90.25 %<br><img src="5.png" alt=""><br><img src="6.png" alt=""><br><img src="7.png" alt=""><br><img src="8.png" alt=""></p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. </li>
<li>The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to  fits the data well without also incurring noticable overfitting.</li>
<li>You will also learn later about regularization, which lets you use very large models (such as n_h = 50) without much overfitting. </li>
</ul>
<p><strong>Optional questions</strong>:</p>
<p><strong>Note</strong>: Remember to submit the assignment but clicking the blue “Submit Assignment” button at the upper-right. </p>
<p>Some optional/ungraded questions that you can explore if you wish: </p>
<ul>
<li>What happens when you change the tanh activation for a sigmoid activation or a ReLU activation?</li>
<li>Play with the learning_rate. What happens?</li>
<li>What if we change the dataset? (See part 5 below!)</li>
</ul>
<font color='blue'>
**You've learnt to:**
- Build a complete neural network with a hidden layer
- Make a good use of a non-linear unit
- Implemented forward propagation and backpropagation, and trained a neural network
- See the impact of varying the hidden layer size, including overfitting.


<p>Nice work! </p>
<h2 id="5-Performance-on-other-datasets"><a href="#5-Performance-on-other-datasets" class="headerlink" title="5) Performance on other datasets"></a>5) Performance on other datasets</h2><p>If you want, you can rerun the whole notebook (minus the dataset part) for each of the following datasets.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Datasets</span><br><span class="line">noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure &#x3D; load_extra_datasets()</span><br><span class="line"></span><br><span class="line">datasets &#x3D; &#123;&quot;noisy_circles&quot;: noisy_circles,</span><br><span class="line">            &quot;noisy_moons&quot;: noisy_moons,</span><br><span class="line">            &quot;blobs&quot;: blobs,</span><br><span class="line">            &quot;gaussian_quantiles&quot;: gaussian_quantiles&#125;</span><br><span class="line"></span><br><span class="line">### START CODE HERE ### (choose your dataset)</span><br><span class="line"># dataset &#x3D; &quot;noisy_moons&quot;</span><br><span class="line">dataset&#x3D;&quot;blobs&quot;</span><br><span class="line">### END CODE HERE ###</span><br><span class="line"></span><br><span class="line">X, Y &#x3D; datasets[dataset]</span><br><span class="line">X, Y &#x3D; X.T, Y.reshape(1, Y.shape[0])</span><br><span class="line"></span><br><span class="line"># make blobs binary</span><br><span class="line">if dataset &#x3D;&#x3D; &quot;blobs&quot;:</span><br><span class="line">    Y &#x3D; Y%2</span><br><span class="line"></span><br><span class="line"># Visualize the data</span><br><span class="line">plt.scatter(X[0, :], X[1, :], c&#x3D;Y, s&#x3D;40, cmap&#x3D;plt.cm.Spectral);</span><br></pre></td></tr></table></figure>

<p><img src="9.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/10/coursera%E7%BB%83%E4%B9%A0%E2%80%94%E2%80%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BB%BA%E7%AB%8B/" data-id="ck7t8tree0001k6s6ggnx1iif" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/" rel="tag">deep learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-感想" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/10/%E6%84%9F%E6%83%B3/" class="article-date">
  <time datetime="2018-12-10T08:45:22.000Z" itemprop="datePublished">2018-12-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/10/%E6%84%9F%E6%83%B3/">感想</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>机会，留给有准备的人。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/10/%E6%84%9F%E6%83%B3/" data-id="ck7t8trey0018k6s64cfy7w0g" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BF%83%E6%83%85/" rel="tag">心情</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-如何做研究生" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/12/10/%E5%A6%82%E4%BD%95%E5%81%9A%E7%A0%94%E7%A9%B6%E7%94%9F/" class="article-date">
  <time datetime="2018-12-10T07:27:28.000Z" itemprop="datePublished">2018-12-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/12/10/%E5%A6%82%E4%BD%95%E5%81%9A%E7%A0%94%E7%A9%B6%E7%94%9F/">如何做研究生</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="1.png" alt="施一公"></p>
<p>很多研究生特别是新入学的研究生常常会问：如何成为一名优秀的博士生？相信每个科研工作者对于这一问题都会有自己的看法。施一公先生以其攻读博士研究生和指导了众多优秀研究生的经历发表了对于这一问题的看法，回答非常朴实，特别值得我们一读。本期青塔小编整理了施一公先生发表在科学网博客上的这两篇文章，全文如下：</p>
<p>导读：我从获得博士学位至今已经整整16个春秋，但博士阶段的感受仍然历历在目。我从指导自己独立实验室的第一个博士生到现在也已经13年了，其中的博士研究生和博士后中已经有11人在美国和中国的大学里担任独立实验室的PI。他们的成长过程差别极大，性格、能力也各有不同。应该说，没有任何一个学生可以简单地遵循另外一个优秀科学家的足迹脱颖而出。从这个意义上讲，科学家的成功是不可能复制的。但是，优秀科学家常常具备的共同特点应该对年轻学生有很大启发。 </p>
<p>本文主要来自我在清华大学研究生入学教育里一次2.5小时的讲座，又综合了一些随后的思考和总结。在那次讲座中，我一再强调，我的目的不是要求研究生完全按照我讲的去做，而是希望从根本上冲击、振荡一下研究生的思考角度，启发大家找到最适合自己的成才之路。</p>
<p>1．时间的付出。 </p>
<p>所有成功的科学家一定具有的共同点，就是他们必须付出大量的时间和心血。这是一条真理。实际上，无论社会上哪一种职业，要想成为本行业中的佼佼者，都必须付出比常人多的时间。有时，个别优秀科学家在回答学生或媒体的问题时，轻描淡写地说自己的成功凭借的是运气，不是苦干。这种回答其实不够客观、也有些不负责任，因为他们有意忽略了自己在时间上的大量付出，而只是强调成功过程中的一个偶然因素，这样说的效果常常对年轻学生造成很大的误导，因为有些幼稚的学生甚至会因此开始投机取巧、不全力进取而是等待所谓的运气；另外一些学生则开始寻找他们的运气，把相当一部分精力和时间用在了与科学研究无关的事情上面。说极端一点：如果真有这样主要凭运气而非时间付出取得成功的科学家，那么他的成功很可能是攫取别人的成果，而自己十有八九不具备真正在领域内领先的学术水平。 </p>
<p>大约在十年前，著名的华人生物学家蒲慕明先生曾经有一封非常著名的email在网上广为流传，这封email是蒲先生写给自己实验室所有博士生和博士后的，其中的观点我完全赞同。这封email写的语重心长，从中可以看出蒲先生的良苦用心。我无论是在普林斯顿还是在清华大学都把这封email转给了我实验室的所有学生，让他们体会。其中的一段是这样说的：“The most important thing is what I consider to be sufficient amount of time and effort in the lab work. I mentioned that about 60 hr working time per week is what I consider the minimal time an average successful young scientist in these days has to put into the lab work……I suggest that everyone puts in at least 6 hr concentrated bench work and 2+ hr reading and other research-related activity each day. Reading papers and books should be done mostly after work.”(我认为最重要的事情就是在实验室里的工作时间，当今一个成功的年轻科学家平均每周要有60小时左右的时间投入到实验室的研究工作……我建议每个人每天至少有6小时的紧张实验操作和两小时以上的与科研直接有关的阅读等。文献和书籍的阅读应该在这些工作时间之外进行。)。 </p>
<p>有些学生读完蒲先生的email后告诉我，“看来我不是做学术的料，因为我真的吃不起这份苦。”我常常回复道，“我在你这么大年纪的时候，也会觉得长期这样工作不可思议。但在不知不觉之中，你会逐渐被科学研究的精妙所打动，也会为自己的努力和成绩骄傲，你会逐渐适应这种生活方式！”这句话表面上是劝学生，实则是我自己的经历与体会。 </p>
<p>我从小就特别贪玩，并不喜欢学习。但来自学校和父母的教育与压力迫使自己尽量刻苦读书；我高中就读于河南省实验中学，凭借着比别人更加刻苦的努力，综合成绩始终名列前茅。1984年全国高中数学联赛我获得河南赛区第一名，保送进入清华大学。大学阶段，我保持了刻苦的传统，综合成绩全班第一并提前一年毕业。但这种应试和灌输教育的结果就是我很少真正独立思考、对专业不感兴趣。大学毕业时，我本没有打算从事科学研究，而是一心一意想下海经商。阴差阳错之间，我踏上了赴美留学之路。 </p>
<p>可想而知，留学的第一年，我情绪波动很大，内心浮躁而迷茫，根本无心念书、做研究，而是花了很多时间在中餐馆打工、选修计算机课程。第二年，我开始逐渐适应科研的“枯燥”，并开始有了一点自己的体会，有时领会了一些精妙之处后会洋洋得意，也会产生“原来不过如此”的想法，逐渐对自己的科研能力有了一点自信。这期间，博士研究生的课程全部修完，我每周五天、每天从上午9点做实验到晚上7、8点，周末也会去两个半天。到了第三年，我已经开始领会到科研的逻辑，有点儿跃跃欲试的感觉，在组会上常常提问，而这种“入门”的感觉又让我对研究增加了很多兴趣，晚上常常干到11点多，赶最后一班校车从霍普金斯医学院回Homewood campus（我住在附近）。1993年我曾经在自己的实验记录本的日期旁标注“This is the 21st consecutive day of working in the lab.”（这是我连续第21天在实验室工作。），以激励自己。其实，这多少有作秀之嫌，因为其中的一个周末我一共只做了五、六个小时的实验。到第四年以后，我完全适应了实验室的科研环境，也不会再感受到枯燥或时间上的压力了。时间安排完全服从实验的需要，尽量往前赶。其实，这段时期的实验时间远多于刚刚进实验室的时候，但感觉上好多了。 </p>
<p>研究生阶段后期，我的刻苦在实验室是出了名的。在纽约做博士后时期则是我这辈子最苦的两年，每天晚上做实验到半夜三点左右，回到住处躺下来睡觉时常常已是四点以后；但每天早晨八点都会被窗外纽约第一大道(First Avenue)上的汽车喧闹声吵醒，九点左右又回到实验室开始了新的一天。每天三餐都在实验室，分别在上午9点、下午3点和晚上9、10点。这样的生活节奏持续11天，从周一到第二个星期的周五，周五晚上做灰狗长途汽车回到巴尔地摩(Baltimore)的家里，周末两天每天睡上近十个小时，弥补过去11天严重缺失的睡眠。周一早晨再开始下一个11天的奋斗。虽然体力上很累，但我心里很满足、很骄傲，我知道自己在用行动打造未来、在创业。有时我也会在日记里鼓励自己。我住在纽约市曼哈顿区65街与第一大道路口附近，离纽约著名的中心公园(Central Park)很近，那里也时有文化娱乐活动，但在纽约工作整整两年，我从未迈进中心公园一步。 </p>
<p>我一定会把自己的这段经历讲给每一个我自己的学生听，新生常常问我：“老师，您觉得自己苦吗？”我通常回答，“只有做自己没有兴趣的事情时候觉得很苦。有兴趣以后一点也不觉得苦。” 是啊，一个精彩的实验带给我的享受比看一部美国大片强多了。现在回想起当时的刻苦，感觉仍很骄傲、很振奋！有时我想：如果自己在博士生、博士后阶段的那七年半不努力进取，而是不加节制地看电影、读小说、找娱乐（当时的互联网远没有现在这么内容丰富），现在该是什么状况？ </p>
<p>做一个优秀的博士生，时间的付出是必要条件。  </p>
<p>2．方法论的转变 </p>
<p>要想在科学研究上取得突破和成功，只有时间的付出和刻苦，是不够的。批判性分析（critical analysis）是必须具备的一种素质。 </p>
<p>研究生与本科生最大的区别是：本科生以吸取学习人类积累的知识为主、兼顾科学研究和技能训练；而博士生的本质是通过科学研究来发掘创造新知识，当前和以往学习的知识都是为了更好地服务于科学研究。在以学习知识为主的本科生阶段，提出问题固然重要，但答案往往已经存在，所以问题是否critical没有那么关键。博士生阶段则完全不同，必须具备critical analysis的能力，否则不可能成为优秀的科学家。这一点，我称之为方法论的转变。 </p>
<p>其实，整个大学和研究生阶段教育的实质就是培养critical analysis的能力，养成能够进行创新科研的方法论。这里的例子非常多，覆盖的范围也非常广，在此举几个让我终生难忘的例子。 </p>
<p>(1)  正确分析负面结果（negative results）是成功的关键。 </p>
<p>作为生命学科的一名博士生，如果每一个实验都很顺利、能得到预料中的正面结果（positive results），除个别研究领域外，一般只需要6-24个月就应该可以获得博士学位所需要的所有结果了。然而实际上，在美国，生命学科的一个博士研究生，平均需要6年左右的时间才能得到PhD学位。这一数字本身就说明：绝大多数实验结果会与预料不符，或者是负面结果（negative results）。大多数低年级的博士生对负面结果的看法很消极，直接影响了他们critical analysis能力的培养。 </p>
<p>其实，只要有适当的对照实验（control experiments）、判断无误的负面实验结果往往是通往成功的必经之路。一般来说，任何一个探索型课题的每一步进展都有几种、甚至十几种可能的途径（hypothesis），取得进展的过程基本就是排除不正确、找到正确方向的过程，很多情况下也就是将这几种、甚至十几种可能的途径一一予以尝试、排除，直到找到一条可行之路的过程。在这个过程中，一个可信的(conclusive)负面结果往往可以让我们信心饱满地放弃目前这一途径，如果运用得当，这种排除法会确保我们最终走上正确的实验途径。从这个角度讲，负面的实验结果不仅很正常、也很有益于课题的最终成功。 </p>
<p>非常遗憾的是，大多数学生的负面结果并不令人信服，经不起逻辑的推敲！而这一点往往是阻碍科研课题进展的最大阻碍。比如，按照一个常规的 protocol操作时不能得到positive control的相应结果，或者缺乏相应的对照实验，或者是对可信的实验结果在分析和判断上产生了失误，从而做出“负面结果”或“不确定”（inconclusive results）的结论，这种结论对整个课题进展的伤害非常大，常常让学生在今后的实验中不知所措、苦恼不堪。我告诫并鼓励我所有的学生：只要你不断取得conclusive的负面结果，你的课题就会很快走上正路；而在不断分析负面结果的过程中所掌握的强大的逻辑分析能力也会使你也会很快成熟，成长为一名优秀的科学家。 </p>
<p>我对一帆风顺、很少取得负面结果的学生总是很担心，因为他们没有真正经历过科研上critical analysis的训练。在我的实验室，偶尔会有这样的学生只用很短的时间（两年左右，有时甚至一年）就完成了PhD论文所需要的结果；对这些学生，我一定会让他们继续承担一些富有挑战性的新课题，让他们经受负面结果的磨练。没有这些磨练，他们很难真正具备critical analysis的能力，将来也很难成为可以独立领导一个实验室的优秀科学家。 </p>
<p>所以，不要害怕负面结果，关键是如何从分析负面结果中获取正确的信息。 </p>
<p>(2)  耗费时间的完美主义阻碍创新进取。 </p>
<p>Nikola Pavletich是我的博士后导师，也是对我影响最大的科学家之一，他有着极强的实验判断力和思维能力，做出了一系列包括p53、Rb、CDK complex、SCF complex、BRCA1等在内的里程碑式的研究工作，享誉世界结构生物学界，31岁时即升任正教授。1996年4月，我刚到Nikola实验室不久，纯化一个表达量相当高的蛋白Smad4，两天下来，蛋白虽然纯化了，但结果很不理想：得到的产量可能只有应该得到的20%左右。见到Nikola，我不好意思地说：产率很低，我计划继续优化蛋白的纯化方法，提高产率。他反问我：（大意）Why do you want to improve the yield? Don’t you have enough protein for crystallization trials? （你为什么想提高产率？已有的蛋白不够你做初步的结晶实验吗？）我回敬道：I do have enough protein for crystallization screen. But I need to optimize the yield first so that I can accumulate more materials.（我有足够的蛋白做结晶筛选，但我需要优化产率以得到更多的蛋白。） 他不客气地打断我：No. The yield is high enough. Your time is more important than yield. Please proceed to crystallization. （不对。产率够高了，你的时间比产率重要。请尽快开始结晶。）实践证明了Nikola建议的价值。我用仅有的几毫克蛋白进行crystallization screen,很快意识到这个construct并不理想，需要通过protein engineering除去其N-端较flexible的几十个氨基酸。而除去N-端几十个氨基酸的蛋白不仅表达量高、而且生化性质稳定，纯化起来非常容易，根本不用担心产率的问题。 </p>
<p>在大刀阔斧进行创新实验的初期阶段，对每一步实验的设计当然要尽量仔细，但一旦按计划开始后对其中间步骤的实验结果不必追求完美，而是应该义无反顾地把实验一步步推到终点，看看可否得到大致与假设相符的总体结果。如果大体上相符，你才应该回过头去仔细地再改进每一步的实验设计。如果大体不符，而总体实验设计和操作都没有错误，那你的假设（或总体方向）很可能是有大问题的。 </p>
<p>这个方法论在每一天的实验中都会用到。比如，结构生物学中，第一次尝试纯化一种新的蛋白不应该追求每一步的产率，而应该尽量把所有纯化步骤进行到底，看看能否拿到适于结晶的蛋白。第一次尝试limited proteolysis，不应该刻意确定protease浓度或追求蛋白纯度，而是要关注结果中是否有protease-resistant core domain。从1998年开始自己的独立实验室到现在，我告诉所有学生：切忌一味追求完美主义。 </p>
<p>我把这个方法论推到极限：只要一个实验还能往前走，一定要做到终点，尽量看到每一步的结果，之后需要时再回头看，逐一解决中间遇到的问题。 </p>
<p>(3)  科研文献（literature）与学术讲座（seminar） 的取与舍。 </p>
<p>Nikola Pavletich博学多才。在我们许多博士后的心目中，他一定读很多文章、常常去听seminar。没想到，我最大的惊讶出现在我笃信无疑的这一点。 </p>
<p>在我的博士生阶段，我的导师Jeremy Berg非常重视相关科研文献的阅读，有每周一次的组内journal club，讨论重要的科研进展。刚到Nikola实验室，我曾试图表现一下自己读paper的功底、也想同时与Nikola讨论以得到他的真传。96年春季的一天，我精读了一篇《Nature》article，午饭前遇到Nikola，向他描述这篇文章的精妙，同时期待着他的评述。Nikola面色有点尴尬地对我说：对不起，我还没看过这篇文章。噢，也许这篇文章太新，他还没有来得及读。过了几天，我因故阅读了一篇几个月前发表的《Science》research article，又去找Nikola讨论，没想到他又说没看过。几次碰壁之后，我不解地问Nikola：You know so much. You must read a lot of papers. Why is it that you didn’t read the ones I read?（你知识如此渊博，一定是广泛阅读了大量文献。你为什么恰好没有读我提到的这几篇论文呢？） Nikola看着我说: I don’t read a lot. （我阅读不广泛。）我反问： If you don’t read a lot, how can you be so good at research? And how can you reference so many papers in your own publications?（如果你不广泛阅读，你的科研怎么会这么好？你怎么能在自己的论文里引用这么多文献？）  Nikola的回答让我彻底意外：（大意）I only read papers that are directly relevant to my research interests, and I only read more papers when it comes to writing my own papers（我只读与我的研究兴趣有直接关系的论文。并且只有在我写论文时才会大量阅读。） </p>
<p>我做博士后的单位Memorial Sloan-Kettering Cancer Center有很好的系列学术讲座(Institute Seminar Series)，常常会请来各个生命科学领域的大牛来演讲。有一次，一个诺贝尔奖得主来讲Institute seminar,并且点名要与Nikola交谈。在绝大多数人看来，这可是一个不可多得的好机会去接近大人物、取得好印象。Nikola告诉他的秘书：请你替我转达我的歉意，seminar那天我恰好不在。我们也为Nikola遗憾。让我万万想不到的是，诺贝尔奖得主seminar的那天,Nikola把自己关在办公室里，早晨来了以后直到傍晚一直没有出门，当然也没有去听讲座。当然，这也许是巧合 – Nikola取消了他的出行计划；但以我们对Nikola的了解，他十有八九是在写paper。后来，我们也意识到，这样的事情发生在Nikola身上已经见多不怪了。 </p>
<p>在我离开Nikola实验室前，我带着始终没有完全解开的谜，问他：如果你不怎么读papers，又不怎么去听讲座，你怎么还能做一个如此出色的科学家？他回答说：（大意）我的时间有限，每天只有10小时左右在实验室，权衡利弊之后，我只能把我的有限时间用在我认为最重要的事情上，如解析结构、分析结构、与学生讨论课题、写文章。如果没有足够的时间，我只能少读文章、少听讲座了。 </p>
<p>Nikola的回答表述了一个简单的道理：一个人必须对他做的事情做些取舍，不可能面面俱到。无论是科研文献的阅读还是学术讲座的听取，都是为了借鉴相关经验、更好地服务于自己的科研课题。 </p>
<p>在博士生阶段，尤其是前两年，我认为必须花足够的时间去听各相关领域的学术讲座、并进行科研文献的广泛阅读，打好critical thinking的基础；但随着科研课题的深入，对于文献阅读和学术讲座就需要有一定的针对性，也要开始权衡时间的分配了。 </p>
<p>(4) 挑战传统思维 </p>
<p>从我懂事开始，就受到教育：凡事失败都有其道理，应该找到失败的原因后再重新开始尝试。直到1996年，我在实验上也遵循这一原则。但在Nikola 的实验室，这一基本原则也受到有理有据的挑战。 </p>
<p>有一次，一个比较复杂的实验失败了。我很沮丧，准备花几天时间多做一些control实验找到问题所在。没想到，Nikola阻止了我，他皱着眉头问我， （大意）Tell me why you want to figure out why your experiment failed? （告诉我你为什么要搞明白实验为何失败？）我觉得这个问题太没道理，理直气壮地回答：I need to know what went wrong so that I can get it to work next time.  （我得知道哪里错了才能保证下一次可以成功。）Nikola马上评论道：（大意）You don’t need to. All you need to do is to carefully repeat your experiment and hopefully it will work next time. Many times figuring out why your previous experiment failed will take much longer time than simply repeating your experiment. For a sophisticated, one-time experiment, the best solution to a failed experiment is to repeat it carefully.（不需要。你真正要做的是把实验重复一遍，也许下次就可以做成。与其花大把时间搞清楚一个实验为何失败，不如先重复一遍。面对一个失败了的复杂的一次性实验，最好的办法就是认认真真重新做一次。）  后来，Nikola又把他的观点升华: （大意）It is a philosophical decision whether to figure out why an experiment failed. The conventional wisdom of understanding every glitch may not represent the best approach.仔细想想，这些话很有道理。并不是所有失败的实验都一定要找到其原因，尤其是生命科学的实验，过程繁琐复杂；大部分失败的实验是由简单的操作错误引起的，比如PCR忘记加某种成分了，可以仔细重新做一遍；这样往往可以解决问题。只有那些关键的、不找到失败原因就无法前行的实验才需要刨根究源。 </p>
<p>我选择的这些例子多少有点“极端”，但只有这样才能更好地起到震荡大家思维的作用。其实，在我自己的实验室里，这几个例子早已经给所有学生反复讲过多次了，而且每次讲完之后，我都会告诉大家打破迷信、怀疑成规，而关键的关键是：Follow logic！（跟着逻辑走！）  </p>
<p>我每天在实验室里注定会重复讲的一句话就是：Follow logic！每天对不同的学生讲，加在一起至少有5遍以上吧。而我自己每次与博士生讨论课题也总是遵循严密的逻辑，用推理、排除法找到实验的下一步解决方案。 </p>
<p>严密的逻辑是 critical analysis的根本。 </p>
<p>（（本文由施一公先生发表于科学网博客，青塔已获授权转载）copy from     青塔）</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/12/10/%E5%A6%82%E4%BD%95%E5%81%9A%E7%A0%94%E7%A9%B6%E7%94%9F/" data-id="ck7t8trez001ak6s63woo1dr2" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BF%83%E6%83%85/" rel="tag">心情</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-一题多解之非常规解法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/25/%E4%B8%80%E9%A2%98%E5%A4%9A%E8%A7%A3%E4%B9%8B%E9%9D%9E%E5%B8%B8%E8%A7%84%E8%A7%A3%E6%B3%95/" class="article-date">
  <time datetime="2018-11-25T09:39:12.000Z" itemprop="datePublished">2018-11-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/11/25/%E4%B8%80%E9%A2%98%E5%A4%9A%E8%A7%A3%E4%B9%8B%E9%9D%9E%E5%B8%B8%E8%A7%84%E8%A7%A3%E6%B3%95/">一题多解之非常规解法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/25/%E4%B8%80%E9%A2%98%E5%A4%9A%E8%A7%A3%E4%B9%8B%E9%9D%9E%E5%B8%B8%E8%A7%84%E8%A7%A3%E6%B3%95/" data-id="ck7t8trex0013k6s60339ge2v" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/" rel="tag">blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deep-learning/" rel="tag">deep learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%88%B7%E9%A2%98/" rel="tag">刷题</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BF%83%E6%83%85/" rel="tag">心情</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BD%AF%E4%BB%B6-photoshop/" rel="tag">软件 photoshop</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/blog/" style="font-size: 10px;">blog</a> <a href="/tags/deep-learning/" style="font-size: 16.67px;">deep learning</a> <a href="/tags/%E5%88%B7%E9%A2%98/" style="font-size: 20px;">刷题</a> <a href="/tags/%E5%BF%83%E6%83%85/" style="font-size: 13.33px;">心情</a> <a href="/tags/%E8%BD%AF%E4%BB%B6-photoshop/" style="font-size: 10px;">软件 photoshop</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/03/16/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2019/03/09/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a>
          </li>
        
          <li>
            <a href="/2019/01/23/leetcode-208/">leetcode-208</a>
          </li>
        
          <li>
            <a href="/2019/01/14/sliding-window/">sliding-window</a>
          </li>
        
          <li>
            <a href="/2019/01/12/leetcode46-permutations/">leetcode46-permutations</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>